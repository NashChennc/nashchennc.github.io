<!DOCTYPE html>
<html lang="cn">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Transformer - NaN&#39;s Blog ｜ Not a Nihilist</title>

  <!-- MDUI CSS -->
  <link rel="stylesheet" href="/mdui/css/mdui.min.css">

  <!-- KaTeX CSS (If needed) -->
  <link rel="stylesheet" href="/katex/katex.min.css">

  <!-- Custom CSS (最小化，只包含无法用 MDUI 实现的部分) -->
  <link rel="stylesheet" href="/css/custom.css">

  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <meta name="theme-color" content="#3F51B5">
<meta name="generator" content="Hexo 8.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body class="mdui-appbar-with-toolbar mdui-theme-primary-indigo mdui-theme-accent-pink mdui-color-grey-100">
  
  <!-- Header/AppBar -->
  <header class="appbar mdui-appbar mdui-appbar-fixed">
    <div class="mdui-toolbar mdui-color-theme">
        
          <div class="mdui-toolbar-spacer"></div>
            <div class="mdui-tab mdui-tab-full-width">
                
                <a href="/" class="mdui-ripple ">
                    Home
                </a>
                
                
                    
                        <a href="/categories/笔记" class="mdui-ripple ">
                            笔记 | 9
                        </a>
                    
                
                    
                        <a href="/categories/科研" class="mdui-ripple ">
                            科研 | 7
                        </a>
                    
                
                    
                        <a href="/categories/碎碎念" class="mdui-ripple ">
                            碎碎念 | 40
                        </a>
                    
                
                    
                        <a href="/categories/工具箱" class="mdui-ripple ">
                            工具箱 | 15
                        </a>
                    
                
                    
                        <a href="/categories/专栏" class="mdui-ripple ">
                            专栏 | 6
                        </a>
                    
                
            </div>
        
        <div class="mdui-toolbar-spacer"></div>
        <a href="/about" class="mdui-btn mdui-btn-icon mdui-ripple" mdui-tooltip="{content: 'About'}">
            <i class="mdui-icon material-icons"> more_vert </i>
        </a>
    </div>
</header>

  <!-- Main Content (Layout control moved to individual views) -->
  <div id="main-content">
    <!-- 全宽 Banner -->
<div class="banner-container" style="height: 320px;">
    <div class="banner-bg" style="background-image: url(/image/banner.jpeg);"></div>
    
    <div class="mdui-card-media-covered banner-overlay" style="background: rgba(0, 0, 0, 0.4);">
        <div class="mdui-container">
            <div class="mdui-card-primary mdui-float-right mdui-text-color-white mdui-p-y-2">
                <div class="mdui-card-primary-title" style="font-weight: 300;">
                     <div style='display:flex; justify-content:center; align-items:flex-end; width:100%; white-space:nowrap;'>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.35em; opacity:0.7; text-transform:uppercase; letter-spacing:1px; margin-right:-1px; line-height:1.2;'>NOUS SOMMES</span> <span style='font-size:1em; font-weight:400; line-height:1.2;'>我们是</span> </div>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.35em; opacity:0.7; text-transform:uppercase; letter-spacing:1px; line-height:1.2; position:relative;'> DES ENFANTS<span style='position:absolute; right:-0.8em;'>,</span> </span> <span style='font-size:1em; font-weight:400; line-height:1.2; position:relative;'> 孩子<span style='position:absolute; right:-1em;'>，</span> </span> </div>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.35em; opacity:0.7; text-transform:uppercase; letter-spacing:1px; margin-right:-1px; line-height:1.2;'>MAIS</span> <span style='font-size:1em; font-weight:400; line-height:1.2;'>但</span> </div>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.35em; opacity:0.7; text-transform:uppercase; letter-spacing:1px; margin-right:-1px; line-height:1.2;'>DES ENFANTS</span> <span style='font-size:1em; font-weight:400; line-height:1.2;'>我们</span> </div>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.35em; opacity:0.7; text-transform:uppercase; letter-spacing:1px; line-height:1.2; position:relative;'> PROGRESSIFS<span style='position:absolute; right:-0.8em;'>,</span> </span> <span style='font-size:1em; font-weight:400; line-height:1.2; position:relative;'> 精力充沛<span style='position:absolute; right:-1em;'>，</span> </span> </div>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.35em; opacity:0.7; text-transform:uppercase; letter-spacing:1px; margin-right:-1px; line-height:1.2;'>PLEINS DE FORCE ET DE COURAGE</span> <span style='font-size:1em; font-weight:400; line-height:1.2;'>勇往直前<span style='position:absolute; right:-1em;'>。</span> </span> </div>
</div>
                </div>
                <div class="mdui-card-primary-subtitle mdui-float-right" style="opacity: 0.9;">
                     <div style='display:flex; justify-content:center; align-items:flex-end; gap:1.5rem; white-space:nowrap;'>
<div style='display:flex; flex-direction:column; align-items:center;'> <span style='font-size:0.7em; opacity:0.7; letter-spacing:1px; text-transform:uppercase;'>ÉVARISTE GALOIS (1811-1832)</span> <span style='font-size:1.1em; font-weight:400; line-height:0.8; position:relative;'> <span style='position:absolute; left:-2em;'>—— </span>埃瓦里斯特 · 伽罗瓦 </span> </div>
<div style='padding-bottom:1.6px; font-size:0.9em; opacity:0.8; font-weight:300; letter-spacing:0.5px; position:relative; padding:0 0.6em;'> ，1831年 于巴黎政治审判庭上的辩护 </div>
</div>
                </div>
            </div>
        </div>
    </div>
</div>
<!-- FAB -->
<div id="exampleFab" class="mdui-fab-wrapper" mdui-fab="{trigger: 'hover'}">
    <a class="mdui-fab mdui-ripple mdui-color-theme-accent" href="/archives" mdui-tooltip="{content: 'goto Archive', position: 'left'}">
        <i class="mdui-icon material-icons">bookmark</i>
        <i class="mdui-icon mdui-fab-opened material-icons">bookmark</i>
    </a>
    <div class="mdui-fab-dial">
        <a href="/about" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-teal">
            <i class="mdui-icon material-icons">more_vert</i>
        </a>
        <a href="#top" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-blue-700" mdui-tooltip="{content: 'goto top', position: 'left'}">
            <i class="mdui-icon material-icons">keyboard_arrow_up</i>
        </a>
        <a href="#page-bottom" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-indigo" mdui-tooltip="{content: 'goto bottom', position: 'left'}">
            <i class="mdui-icon material-icons">keyboard_arrow_down</i>
        </a>
    </div>
</div>

<div class="mdui-container">
    <div class="mdui-row mdui-p-t-1 mdui-p-b-1">
        <!-- 文章内容 -->

        <div class="mdui-col-md-9 mdui-col-xs-12 mdui-p-t-1 mdui-p-b-4">
            <div class="mdui-card mdui-hoverable">
                <div class="mdui-card-primary mdui-p-x-5">
                    <div class="mdui-card-primary-title mdui-typo-display-1">
                        Transformer
                    </div>
                    <div class="mdui-row mdui-p-t-2">
                        <div class="mdui-col-xs-9 mdui-typo-caption-opacity">
                            Last Update: 2026/01/19 03:40
                        </div>
                        <div class="mdui-col-xs-3">
                            <div class="mdui-float-right mdui-typo">
                                
                                    <a href="/categories/%E7%AC%94%E8%AE%B0/">
                                        笔记
                                    </a>
                                
                            </div>
                        </div>
                    </div>
                    <div class="mdui-divider"></div>
                    <br/>
                </div>
                <div class="mdui-card-content mdui-typo mdui-p-x-5">
                    <p>这是一个非常深刻且准确的直觉。从数学本质上讲，自注意力机制（Self-Attention）确实是一个<strong>作用于集合上的、动态权重的聚合函数（Aggregation Function）</strong>。</p>
<p>如果不考虑线性投影（Linear Projections）的可学习参数，自注意力机制的核心就是在计算<strong>加权平均（Weighted Average）</strong>，或者更严谨地说是<strong>凸组合（Convex Combination）</strong>。</p>
<p>下面我用数学语言为你推导这个过程，展示它是如何将一个集合的信息“聚合”起来的。</p>
<hr>
<h3 id="1-定义空间与输入">1. 定义空间与输入</h3>
<p>假设我们有一个输入序列（例如一个句子中的单词嵌入），我们可以将其视为一个向量集合（Set of Vectors）：</p>
<p>$$\mathcal{X} = { \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n }$$</p>
<p>其中 $\mathbf{x}<em>i \in \mathbb{R}^{d</em>{model}}$。</p>
<h3 id="2-映射到特征空间-Projections">2. 映射到特征空间 (Projections)</h3>
<p>为了进行聚合，我们需要定义三个角色：</p>
<ul>
<li>
<p><strong>Query (查询)</strong>: “我要找什么？”</p>
</li>
<li>
<p><strong>Key (键)</strong>: “你是谁？”（用于匹配）</p>
</li>
<li>
<p><strong>Value (值)</strong>: “你的内容是什么？”（用于聚合）</p>
</li>
</ul>
<p>对于集合中的每一个元素 $\mathbf{x}_i$，我们通过线性变换将其映射到这三个空间：</p>
<p>$$\mathbf{q}_i = \mathbf{x}_i W^Q, \quad \mathbf{k}_i = \mathbf{x}_i W^K, \quad \mathbf{v}_i = \mathbf{x}_i W^V$$</p>
<p>其中 $W^Q, W^K \in \mathbb{R}^{d_{model} \times d_k}$ 和 $W^V \in \mathbb{R}^{d_{model} \times d_v}$ 是可学习的参数矩阵。</p>
<h3 id="3-定义相关性度量-Aggregation-Weights">3. 定义相关性度量 (Aggregation Weights)</h3>
<p>聚合的核心在于<strong>权重</strong>的分配。在自注意力中，权重是动态生成的，取决于 Query 和 Key 的相似度。</p>
<p>对于第 $i$ 个元素，我们想知道它如何聚合上下文信息。我们计算 $\mathbf{q}_i$ 与所有 $\mathbf{k}_j$ 的内积（Scaled Dot-Product）：</p>
<p>$$e_{ij} = \frac{\mathbf{q}_i \cdot \mathbf{k}_j^T}{\sqrt{d_k}}$$</p>
<p>为了使这些分数构成一个合法的概率分布（即满足聚合函数的性质），我们使用 Softmax 函数进行归一化，得到<strong>注意力权重</strong> $\alpha_{ij}$：</p>
<p>$$\alpha_{ij} = \text{softmax}(e_{i\cdot})<em>j = \frac{\exp(e</em>{ij})}{\sum_{m=1}^n \exp(e_{im})}$$</p>
<p>这里有两个关键的数学性质：</p>
<ol>
<li>
<p>$\alpha_{ij} \ge 0$ (非负性)</p>
</li>
<li>
<p>$\sum_{j=1}^n \alpha_{ij} = 1$ (归一性)</p>
</li>
</ol>
<h3 id="4-聚合函数-The-Aggregation">4. 聚合函数 (The Aggregation)</h3>
<p>现在我们有了权重 $\alpha_{ij}$ 和内容 $\mathbf{v}_j$。</p>
<p>第 $i$ 个位置的输出 $\mathbf{z}_i$ 仅仅是所有 Values 的加权和：</p>
<p>$$\mathbf{z}<em>i = \sum</em>{j=1}^n \alpha_{ij} \mathbf{v}_j$$</p>
<h3 id="5-几何解释：凸组合-Convex-Combination">5. 几何解释：凸组合 (Convex Combination)</h3>
<p>在数学几何上，上述公式表明：<strong>输出向量 $\mathbf{z}_i$ 是所有 Value 向量 ${\mathbf{v}_1, \dots, \mathbf{v}_n}$ 在特征空间中的一个凸组合。</strong></p>
<ul>
<li>
<p>想象在 $d_v$ 维空间中，点 $\mathbf{v}_1, \dots, \mathbf{v}_n$ 构成了一个多面体。</p>
</li>
<li>
<p>$\mathbf{z}_i$ 一定落在这些点构成的**凸包（Convex Hull）**内部。</p>
</li>
<li>
<p>具体落在哪里？取决于 $\mathbf{q}_i$ 更“关注”哪个 $\mathbf{k}_j$。如果 $\mathbf{q}_i$ 与 $\mathbf{k}<em>3$ 极度相似，那么 $\alpha</em>{i3} \approx 1$，此时 $\mathbf{z}_i \approx \mathbf{v}_3$。</p>
</li>
</ul>
<h3 id="6-矩阵形式-Global-Aggregation">6. 矩阵形式 (Global Aggregation)</h3>
<p>为了高效计算，我们将上述过程写成著名的矩阵形式。</p>
<p>令 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 分别为所有 token 堆叠而成的矩阵。整个集合的自注意力计算就是：</p>
<p>$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \underbrace{\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)}_{\text{Aggregation Matrix } \mathbf{A}} \mathbf{V}$$</p>
<p>这里，矩阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$ 是一个行随机矩阵（Row-stochastic Matrix），每一行代表一个特定的聚合策略。</p>
<hr>
<h3 id="总结：它与传统聚合的区别">总结：它与传统聚合的区别</h3>
<p>普通的聚合函数通常是静态的（如 Mean Pooling, Max Pooling）：</p>
<ul>
<li>
<p><strong>Mean Pooling:</strong> $\mathbf{z} = \frac{1}{n} \sum \mathbf{x}_j$ （权重固定为 $1/n$）</p>
</li>
<li>
<p><strong>Self-Attention:</strong> $\mathbf{z}<em>i = \sum \alpha</em>{ij}(\mathbf{x}) \mathbf{v}_j$ （权重 $\alpha$ 是数据的函数，是**数据依赖（Data-dependent）**的）</p>
</li>
</ul>
<p>所以，用数学风格总结：</p>
<blockquote>
<p><strong>自注意力机制是一个将输入集合映射到其特征凸包内部的函数，其映射坐标（重心坐标）由输入元素间的兼容性（Compatibility）动态决定。</strong></p>
</blockquote>
<p>没问题，我们把节奏放慢。多头注意力（Multi-Head Attention）其实并没有引入新的魔法，它只是把我们刚才讨论的“聚合过程”<strong>并行地做了 $h$ 次</strong>，然后把结果拼起来。</p>
<p>如果不理解维度变化，确实容易晕。我们用最严谨的矩阵记号，配合<strong>维度的流动</strong>来一步步拆解。</p>
<hr>
<h3 id="1-直观理解：从“单次聚合”到“并行聚合”">1. 直观理解：从“单次聚合”到“并行聚合”</h3>
<p>回忆一下刚才的结论：单头注意力是在做一个<strong>凸组合（加权平均）</strong>。</p>
<ul>
<li>
<p><strong>单头</strong>：大家只用一种标准（比如“语法关系”）来决定谁重要，算出一个平均值。</p>
</li>
<li>
<p><strong>多头</strong>：我们把数据拆分到 $h$ 个不同的**子空间（Subspaces）**里。</p>
<ul>
<li>
<p>Head 1 可能关注“语法”，算出一个聚合结果。</p>
</li>
<li>
<p>Head 2 可能关注“指代关系”，算出另一个聚合结果。</p>
</li>
<li>
<p>…</p>
</li>
<li>
<p>最后，我们把这 $h$ 个结果**拼接（Concatenate）**起来，再做一次融合。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-详细数学推导（带维度追踪）">2. 详细数学推导（带维度追踪）</h3>
<p>假设我们的输入矩阵（比如一个句子的 Embedding）是 $\mathbf{X}$。</p>
<ul>
<li>
<p>$n$: 序列长度（token 数量）</p>
</li>
<li>
<p>$d_{model}$: 模型的维度（例如 512）</p>
</li>
<li>
<p>$\mathbf{X} \in \mathbb{R}^{n \times d_{model}}$</p>
</li>
</ul>
<p>我们需要定义“头”的数量 $h$（例如 8）。为了计算方便，通常每个头的维度 $d_k = d_v = d_{model} / h$（例如 $512/8 = 64$）。</p>
<h4 id="第一步：并行的线性投影-Parallel-Projections">第一步：并行的线性投影 (Parallel Projections)</h4>
<p>我们不再只用一组 $W^Q, W^K, W^V$，而是为每一个头 $i$ ($i=1, \dots, h$) 准备一套<strong>独立</strong>的参数矩阵：</p>
<ul>
<li>
<p>$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$</p>
</li>
<li>
<p>$W_i^K \in \mathbb{R}^{d_{model} \times d_k}$</p>
</li>
<li>
<p>$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$</p>
</li>
</ul>
<p>对于第 $i$ 个头，我们将输入 $\mathbf{X}$ 映射到它的子空间：</p>
<p>$$Q_i = \mathbf{X}W_i^Q, \quad K_i = \mathbf{X}W_i^K, \quad V_i = \mathbf{X}W_i^V$$</p>
<blockquote>
<p>维度检查：</p>
<p>$\mathbf{X}$ 是 $(n \times d_{model})$，乘上 $W$ 是 $(d_{model} \times d_k)$。</p>
<p>结果 $Q_i, K_i, V_i$ 的维度都是 $(n \times d_k)$。</p>
</blockquote>
<h4 id="第二步：独立的注意力聚合-Independent-Aggregation">第二步：独立的注意力聚合 (Independent Aggregation)</h4>
<p>现在，每个头都在自己的小世界里，按照我们前文提到的公式，计算自己的<strong>加权平均</strong>。我们将第 $i$ 个头的输出记为 $\text{head}_i$：</p>
<p>$$\text{head}<em>i = \text{Attention}(Q_i, K_i, V_i) = \underbrace{\text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right)}</em>{\text{权重分布 } A_i} V_i$$</p>
<p>这里最关键的一点是：每个头都有自己独立的权重分布 $A_i$。</p>
<p>Head 1 可能觉得第3个词很重要，Head 2 可能觉得第5个词很重要。</p>
<blockquote>
<p>维度检查：</p>
<p>Attention 矩阵 $A_i$ 是 $(n \times n)$。</p>
<p>$V_i$ 是 $(n \times d_v)$。</p>
<p>乘积 $\text{head}_i$ 的维度是 $(n \times d_v)$。</p>
</blockquote>
<h4 id="第三步：拼接-Concatenation">第三步：拼接 (Concatenation)</h4>
<p>现在我们手头有 $h$ 个矩阵，每个都是 $(n \times d_v)$。我们将它们在特征维度上拼接起来：</p>
<p>$$\text{MultiHeadOutput}_{\text{raw}} = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)$$</p>
<blockquote>
<p>维度检查：</p>
<p>既然 $d_v = d_{model} / h$，那么拼接 $h$ 个这样的向量：</p>
<p>最终宽度变回 $h \times d_v = d_{model}$。</p>
<p>拼接后的矩阵维度恢复为 $(n \times d_{model})$。</p>
</blockquote>
<h4 id="第四步：最终的线性融合-Final-Linear-Projection">第四步：最终的线性融合 (Final Linear Projection)</h4>
<p>拼接只是简单地把大家的结果摆在一起，并没有进行信息的交互。所以最后需要乘上一个输出矩阵 $W^O$，把不同子空间的信息混合一下：</p>
<p>$$\text{Output} = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O$$</p>
<p>其中 $W^O \in \mathbb{R}^{d_{model} \times d_{model}}$。</p>
<blockquote>
<p>最终维度：</p>
<p>$(n \times d_{model}) \times (d_{model} \times d_{model}) = \mathbf{(n \times d_{model})}$。</p>
<p>输入和输出的维度完全一致。</p>
</blockquote>
<hr>
<h3 id="3-完整的矩阵形式总结">3. 完整的矩阵形式总结</h3>
<p>如果把上述过程写成一个宏大的数学公式，就是这样：</p>
<p>$$ \text{MultiHead}(\mathbf{X}) = \begin{bmatrix}</p>
<p>\text{Attention}(\mathbf{X}W_1^Q, \mathbf{X}W_1^K, \mathbf{X}W_1^V) \</p>
<p>\vdots \</p>
<p>\text{Attention}(\mathbf{X}W_h^Q, \mathbf{X}W_h^K, \mathbf{X}W_h^V)</p>
<p>\end{bmatrix}_{\text{concat}} W^O $$</p>
<h3 id="4-几何视角的修正">4. 几何视角的修正</h3>
<p>回到我们之前的“聚合函数”直觉，多头注意力意味着什么？</p>
<ol>
<li>
<p><strong>单头</strong>：输出点 $z$ 必须落在 $V$ 定义的凸包（多面体）内部。</p>
</li>
<li>
<p><strong>多头</strong>：</p>
<ul>
<li>
<p>我们先构建了 $h$ 个不同的特征空间（平行宇宙）。</p>
</li>
<li>
<p>在每个宇宙里，我们都根据那个宇宙的规则算出一个重心（凸组合）。</p>
</li>
<li>
<p>最后，我们将这 $h$ 个重心拿回主宇宙，拼在一起，再通过 $W^O$ 旋转/缩放一下。</p>
</li>
</ul>
</li>
</ol>
<p>这就好比为了理解一篇文章，你请了8个专家。</p>
<ul>
<li>
<p>专家1（语法专家）聚合出语法特征。</p>
</li>
<li>
<p>专家2（情感专家）聚合出情感特征。</p>
</li>
<li>
<p>…</p>
</li>
<li>
<p>最后你作为老板（$W^O$），把这8份报告汇总成一份最终报告。
这是一个非常棒的思维练习。将 CNN、RNN 和 Self-Attention 统一在**“聚合函数”（Aggregation Function）**的框架下，能让你一针见血地看到它们的本质区别。</p>
</li>
</ul>
<p>为了实现“处处对应”，我们需要定义一个通用的聚合范式，然后看这三个模型分别是如何填空的。</p>
<hr>
<h3 id="通用聚合范式-The-General-Aggregation-Framework">通用聚合范式 (The General Aggregation Framework)</h3>
<p>假设我们要计算第 $i$ 个位置的输出 $\mathbf{z}_i$，通用的聚合公式可以写为：</p>
<p>$$\mathbf{z}<em>i = \text{Agg}</em>{\mathcal{N}} \left( { \phi(\mathbf{x}_i, \mathbf{x}_j) \cdot \mathbf{x}_j \mid j \in \mathcal{N}(i) } \right)$$</p>
<p>我们需要对比三个核心要素：</p>
<ol>
<li>
<p><strong>邻域 (Neighborhood $\mathcal{N}(i)$)</strong>: 聚合谁的信息？（搜索空间）</p>
</li>
<li>
<p><strong>相关性/权重 ($\phi(\mathbf{x}_i, \mathbf{x}_j)$)</strong>: 聚合时的权重由谁决定？（通过内容还是位置？）</p>
</li>
<li>
<p><strong>参数性质</strong>: 权重是固定的参数，还是运行时计算的变量？</p>
</li>
</ol>
<hr>
<h3 id="1-卷积神经网络-CNN-局部、静态、位置依赖">1. 卷积神经网络 (CNN): 局部、静态、位置依赖</h3>
<p>在 CNN 中，我们将卷积核（Kernel）看作是聚合权重的来源。</p>
<ul>
<li>
<p>邻域 ($\mathcal{N}(i)$): 局部固定窗口 (Local Fixed Window)。</p>
<p>$\mathcal{N}(i) = {j \mid |i - j| \le k}$，即位置 $i$ 周围的 $k$ 个邻居。它是受限的，无法像 Attention 那样一眼看到全局。</p>
</li>
<li>
<p>权重 ($\phi$): 静态、位置依赖 (Static, Position-dependent)。</p>
<p>权重并不取决于 $\mathbf{x}_i$ 和 $\mathbf{x}_j$ 长什么样（内容），而完全取决于 $j$ 相对 $i$ 在哪里。</p>
</li>
<li>
<p>数学形式:</p>
<p>假设卷积核权重为 $\mathbf{W}$，则：</p>
<p>$$\mathbf{z}<em>i = \sum</em>{\delta = -k}^{k} \mathbf{W}<em>{\delta} \cdot \mathbf{x}</em>{i+\delta}$$</p>
<p>这里 $\mathbf{W}<em>{\delta}$ 是训练好的固定参数。不管输入图片是猫还是狗，针对“左边邻居”的权重永远是 $\mathbf{W}</em>{-1}$。</p>
</li>
</ul>
<blockquote>
<p><strong>对应点总结</strong>: CNN 是在做一个<strong>固定网格上的加权求和</strong>。它假设“局部性”最重要，且空间结构（上、下、左、右）蕴含了关键信息。</p>
</blockquote>
<hr>
<h3 id="2-循环神经网络-RNN-时序、递归、历史依赖">2. 循环神经网络 (RNN): 时序、递归、历史依赖</h3>
<p>RNN 的聚合方式非常特殊，它是通过**递归（Recursion）**来隐式地聚合历史信息。</p>
<ul>
<li>
<p>邻域 ($\mathcal{N}(i)$): 单向全历史 (Unidirectional History)。</p>
<p>理论上 $\mathcal{N}(i) = {1, 2, \dots, i-1}$。但它不是直接访问这些邻居，而是通过“上一个隐藏状态” $\mathbf{h}_{i-1}$ 来压缩获取。</p>
</li>
<li>
<p>权重 ($\phi$): 静态、递归共享 (Static, Recurrently Shared)。</p>
<p>RNN 使用同一组权重矩阵（$W, U$）在所有时间步反复运算。</p>
</li>
<li>
<p>数学形式:</p>
<p>$$\mathbf{h}_i = \sigma(\mathbf{W} \mathbf{x}<em>i + \mathbf{U} \mathbf{h}</em>{i-1})$$</p>
<p>如果我们把递归展开（Unroll），$\mathbf{h}_i$ 确实是 $\mathbf{x}_1, \dots, \mathbf{x}_i$ 的聚合结果。但权重的衰减是非线性的，且越远的 $\mathbf{x}_j$ 经过的矩阵乘法次数越多（导致梯度消失/爆炸）。</p>
</li>
</ul>
<blockquote>
<p><strong>对应点总结</strong>: RNN 是在做一个<strong>时间维度上的递归压缩</strong>。它假设“顺序”最重要，且最新的信息依赖于之前的压缩记忆。</p>
</blockquote>
<hr>
<h3 id="3-自注意力-Self-Attention-全局、动态、内容依赖">3. 自注意力 (Self-Attention): 全局、动态、内容依赖</h3>
<p>回过头来看 Self-Attention，对比感就非常强烈了。</p>
<ul>
<li>
<p>邻域 ($\mathcal{N}(i)$): 全局 (Global)。</p>
<p>$\mathcal{N}(i) = {1, 2, \dots, n}$。无论序列多长，位置 $i$ 可以直接“看见”位置 $j$，没有距离限制。</p>
</li>
<li>
<p>权重 ($\phi$): 动态、内容依赖 (Dynamic, Content-dependent)。</p>
<p>这是最大的区别。权重不是训练好的固定参数 $\mathbf{W}$，而是运行时计算出来的 $\alpha_{ij} = \text{softmax}(\mathbf{q}_i \cdot \mathbf{k}_j)$。</p>
<p>如果 $\mathbf{x}_i$ 变了，它关注的对象 $\mathbf{x}_j$ 也会立刻改变。</p>
</li>
<li>
<p>数学形式:</p>
<p>$$\mathbf{z}<em>i = \sum</em>{j=1}^{n} \underbrace{\text{softmax}(\mathbf{x}_i W^Q (\mathbf{x}<em>j W^K)^T)}</em>{\text{Computed at runtime}} (\mathbf{x}_j W^V)$$</p>
</li>
</ul>
<blockquote>
<p><strong>对应点总结</strong>: Self-Attention 是在做一个<strong>基于内容的各种特征向量的凸组合</strong>。它假设“相关性”最重要，位置和距离（在没有 Positional Encoding 前）完全不重要。</p>
</blockquote>
<hr>
<h3 id="4-终极对比表-The-Unified-Comparison-Matrix">4. 终极对比表 (The Unified Comparison Matrix)</h3>
<p>我们将这三种架构放入同一个表格中进行“处处对应”：</p>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>CNN (卷积)</strong></th>
<th><strong>RNN (循环)</strong></th>
<th><strong>Self-Attention (自注意力)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>聚合范围</strong><br><br>  <br><br>(Scope)</td>
<td><strong>局部 (Local)</strong><br><br>  <br><br>$j \in [i-k, i+k]$</td>
<td><strong>递归历史 (Recursive)</strong><br><br>  <br><br>$j &lt; i$ (via hidden state)</td>
<td><strong>全局 (Global)</strong><br><br>  <br><br>$j \in [1, n]$</td>
</tr>
<tr>
<td><strong>权重来源</strong><br><br>  <br><br>(Weight Origin)</td>
<td><strong>模型参数 (Fixed Parameters)</strong><br><br>  <br><br>训练后固定，推理时不因输入而变</td>
<td><strong>模型参数 (Fixed Parameters)</strong><br><br>  <br><br>$W, U$ 随时间共享</td>
<td><strong>数据计算 (Data Dependent)</strong><br><br>  <br><br>$f(\mathbf{x}_i, \mathbf{x}_j)$，随输入动态生成</td>
</tr>
<tr>
<td><strong>寻址方式</strong><br><br>  <br><br>(Addressing)</td>
<td><strong>基于位置 (Location-based)</strong><br><br>  <br><br>“我要看我左边的像素”</td>
<td><strong>基于顺序 (Sequential)</strong><br><br>  <br><br>“我要看上一步的记忆”</td>
<td><strong>基于内容 (Content-based)</strong><br><br>  <br><br>“我要看和我很像的词”</td>
</tr>
<tr>
<td><strong>并行计算</strong><br><br>  <br><br>(Parallelism)</td>
<td><strong>高 (High)</strong><br><br>  <br><br>各个窗口独立计算</td>
<td><strong>低 (Low)</strong><br><br>  <br><br>必须等 $t-1$ 算完才能算 $t$</td>
<td><strong>高 (High)</strong><br><br>  <br><br>矩阵乘法一次搞定所有 $ij$</td>
</tr>
<tr>
<td><strong>长程依赖</strong><br><br>  <br><br>(Long-range)</td>
<td><strong>弱 (Weak)</strong><br><br>  <br><br>需要堆叠多层才能扩大感受野</td>
<td><strong>中 (Medium)</strong><br><br>  <br><br>受限于梯度消失和压缩损耗</td>
<td><strong>强 (Strong)</strong><br><br>  <br><br>路径长度恒为 $O(1)$</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="5-总结">5. 总结</h3>
<p>用一句话概括三者的聚合哲学：</p>
<ul>
<li>
<p><strong>CNN</strong>: “谁在我<strong>旁边</strong>，我就聚合谁，用<strong>固定</strong>的模版。” (Fixed template matching on local grid).</p>
</li>
<li>
<p><strong>RNN</strong>: “谁在我<strong>前面</strong>，我就把它压缩进记忆，用<strong>递归</strong>的方式。” (Sequential compression of history).</p>
</li>
<li>
<p><strong>Self-Attention</strong>: “谁跟我<strong>相关</strong>，我就聚合谁，用<strong>动态</strong>的相似度。” (Dynamic routing based on content similarity).
在现代深度学习工程（尤其是 LLM）中，维度的选择不再是随意的“炼丹”，而是遵循着严格的**Scaling Laws（缩放定律）<strong>和</strong>硬件效率（Hardware Efficiency）**原则。</p>
</li>
</ul>
<p>我们继续沿用之前的符号：$d_{model}$（模型宽度）、$h$（头数）、$d_k$（头维）、$N$（层数/深度）。</p>
<p>以下是现代 Transformer（如 GPT-4, LLaMA, PaLM 等）设定维度的“潜规则”和演变逻辑。</p>
<hr>
<h3 id="1-核心维度的“黄金法则”">1. 核心维度的“黄金法则”</h3>
<h4 id="A-头维度-d-k-：锁定-64-或-128">A. 头维度 ($d_k$)：锁定 64 或 128</h4>
<p>在早期的论文（如 Attention Is All You Need）中，为了保持参数总量不变，$d_k$ 是随着 $h$ 变化的 ($d_k = d_{model}/h$)。</p>
<p>但在现代大模型中，为了适配 GPU 的 Tensor Core 计算效率，$d_k$ 通常被锁定为两个特定数值之一：</p>
<ul>
<li>
<p><strong>$d_k = 64$</strong>: 经典设置（BERT, GPT-2, Original Transformer）。</p>
</li>
<li>
<p><strong>$d_k = 128$</strong>: 现代主流（PaLM, LLaMA, GPT-3）。</p>
</li>
</ul>
<blockquote>
<p>变动逻辑：一旦 $d_k$ 固定（比如 128），随着模型变大（$d_{model}$ 增加），我们只增加头数 $h$。</p>
<p>$$h = \frac{d_{model}}{128}$$</p>
</blockquote>
<h4 id="B-FFN-扩展比-d-ffn-：从-4x-到-GLU-变体">B. FFN 扩展比 ($d_{ffn}$)：从 4x 到 GLU 变体</h4>
<p>Feed-Forward Network (FFN) 负责增加模型的“知识容量”。</p>
<ul>
<li>
<p><strong>传统标准</strong>：$d_{ffn} = 4 \times d_{model}$。</p>
</li>
<li>
<p><strong>现代变体 (SwiGLU)</strong>：LLaMA 等模型使用了 Gated Linear Units (GLU)。为了保持参数量和计算量大致不变，通常将系数调整为 $\frac{2}{3} \times 4 \approx 2.67$，实际中常用 <strong>3.5x 或 $\frac{8}{3}$x</strong> 并取整到 256 的倍数。</p>
</li>
</ul>
<h4 id="C-层数与宽度的比例-N-vs-d-model">C. 层数与宽度的比例 ($N$ vs $d_{model}$)</h4>
<p>没有绝对公式，但根据 <strong>Kaplan Scaling Laws</strong>，当你增加计算预算时，宽度和深度应该同时增加。</p>
<ul>
<li>
<p>通常经验比例：$N \approx \frac{d_{model}}{100} \sim \frac{d_{model}}{64}$（粗略估算，不严谨）。</p>
</li>
<li>
<p>实际趋势：模型越深越难训练，所以现代模型倾向于<strong>变得更“胖”（更宽）而不是极度深</strong>。</p>
</li>
</ul>
<hr>
<h3 id="2-典型量级配置表-The-Standard-Tiers">2. 典型量级配置表 (The Standard Tiers)</h3>
<p>我们可以把模型分为几个标准等级，你会发现大家用的数字惊人地一致（为了复用预训练权重和架构）：</p>
<table>
<thead>
<tr>
<th><strong>模型等级</strong></th>
<th><strong>参数量 (Params)</strong></th>
<th><strong>dmodel​ (Width)</strong></th>
<th><strong>N (Layers)</strong></th>
<th><strong>h (Heads)</strong></th>
<th><strong>dk​ (Head Dim)</strong></th>
<th><strong>代表模型</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Base</strong></td>
<td>~100M</td>
<td>768</td>
<td>12</td>
<td>12</td>
<td>64</td>
<td>BERT-Base, GPT-2 Small</td>
</tr>
<tr>
<td><strong>Large</strong></td>
<td>~300M</td>
<td>1024</td>
<td>24</td>
<td>16</td>
<td>64</td>
<td>BERT-Large, GPT-2 Medium</td>
</tr>
<tr>
<td><strong>XL / 1B</strong></td>
<td>~1.5B</td>
<td>1536 / 2048</td>
<td>24 - 48</td>
<td>16 - 32</td>
<td>64 / 128</td>
<td>GPT-2 XL, Qwen-1.5B</td>
</tr>
<tr>
<td><strong>7B (黄金尺寸)</strong></td>
<td>~7B</td>
<td>4096</td>
<td>32</td>
<td>32</td>
<td>128</td>
<td><strong>LLaMA-2/3-7B</strong>, Mistral-7B</td>
</tr>
<tr>
<td><strong>Huge / 175B</strong></td>
<td>~175B</td>
<td>12288</td>
<td>96</td>
<td>96</td>
<td>128</td>
<td><strong>GPT-3</strong></td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>注意</strong>：LLaMA 架构确立了 <strong>4096 / 32 / 32</strong> (Width/Layers/Heads) 成为 7B 参数量级的工业标准。</p>
</blockquote>
<hr>
<h3 id="3-现代架构的特殊变动-Modern-Innovations">3. 现代架构的特殊变动 (Modern Innovations)</h3>
<p>现代 Transformer（特别是 2023 年后的 LLaMA-3, Mistral 等）对维度做了一些微调，你需要注意以下两个概念，它们改变了维度的计算方式：</p>
<h4 id="A-GQA-Grouped-Query-Attention">A. GQA (Grouped-Query Attention)</h4>
<p>这是为了解决推理速度（KV Cache 显存占用）问题。</p>
<ul>
<li>
<p><strong>传统</strong>：$h_{query} = h_{key} = h_{value}$ (比如都是 32)。</p>
</li>
<li>
<p><strong>GQA</strong>：Query 头数保持 32，但 Key/Value 头数减少（比如只有 8 个）。这意味着 <strong>4 个 Query 头共享 1 个 KV 头</strong>。</p>
</li>
<li>
<p><strong>影响</strong>：$W^K, W^V$ 的参数矩阵维度变小了，变成了原来的 $1/4$（或 $1/G$）。</p>
</li>
</ul>
<h4 id="B-词表扩容-V">B. 词表扩容 ($V$)</h4>
<p>早期的 BERT 词表 $V \approx 30,000$。</p>
<p>现代多语言模型（Qwen, LLaMA-3）为了压缩效率，词表 $V$ 通常扩大到 100,000 - 150,000。</p>
<ul>
<li><strong>影响</strong>：Embedding 矩阵 ($V \times d_{model}$) 变得巨大，占据了相当一部分参数（在小模型中尤为明显）。</li>
</ul>
<hr>
<h3 id="4-总结：如果让你设计一个模型">4. 总结：如果让你设计一个模型</h3>
<p>假设你要设计一个参数量为 $P$ 的模型，现在的标准流程是：</p>
<ol>
<li>
<p><strong>确定 $d_{model}$</strong>：它是核心。比如选 $d_{model}=4096$。</p>
</li>
<li>
<p><strong>确定 $d_k$</strong>：直接锁死 <strong>128</strong>（为了 GPU 效率）。</p>
</li>
<li>
<p><strong>计算 $h$</strong>：$h = 4096 / 128 = 32$。</p>
</li>
<li>
<p><strong>确定 $N$</strong>：参考竞品（如 LLaMA），选 32 层。</p>
</li>
<li>
<p><strong>确定 $d_{ffn}$</strong>：选 $4096 \times 3.5 \approx 14336$ (LLaMA-2 使用 11008，因为使用了 GLU)。</p>
</li>
<li>
<p><strong>微调</strong>：确保所有维度是 <strong>128 或 256 的倍数</strong>（硬件对齐）。</p>
</li>
</ol>
<p>这是一个非常关键的工程视角转换。训练时（Training）我们是“上帝视角”，一次性把所有答案都给模型，它是并行的；但在推理时（Inference），模型变成了“只有当下视角”，必须一个词一个词地蹦出来，这就是 <strong>Auto-regressive（自回归）</strong> 生成。</p>
<p>KV Cache（键值缓存）就是为了让这个“蹦词”的过程不至于慢得像蜗牛。</p>
<p>我们用一个具体的例子：“<strong>The cat is</strong>” -&gt; (预测) “<strong>on</strong>” -&gt; (预测) “<strong>the</strong>” …</p>
<hr>
<h3 id="第一阶段：Prefill（预填充-首词处理）">第一阶段：Prefill（预填充 / 首词处理）</h3>
<p>这是生成过程的第一步，也是唯一一次<strong>并行计算</strong>。哪怕你给的 Prompt 有 1000 个词，这一步也是一次性算完。</p>
<p>假设输入 Prompt 是 <code>[&quot;The&quot;, &quot;cat&quot;, &quot;is&quot;]</code> ($x_1, x_2, x_3$)。</p>
<ol>
<li>
<p>全量计算：</p>
<p>模型并行计算出 $x_1, x_2, x_3$ 的 $Q, K, V$。</p>
<ul>
<li>
<p>$Q_{1:3} = [q_1, q_2, q_3]^T$</p>
</li>
<li>
<p>$K_{1:3} = [k_1, k_2, k_3]^T$</p>
</li>
<li>
<p>$V_{1:3} = [v_1, v_2, v_3]^T$</p>
</li>
</ul>
</li>
<li>
<p>存入 KV Cache：</p>
<p>这是关键点。我们将算好的 $K$ 和 $V$ 存进显存里，别扔掉！</p>
<ul>
<li>
<p><code>Cache_K</code> = $[k_1, k_2, k_3]$</p>
</li>
<li>
<p><code>Cache_V</code> = $[v_1, v_2, v_3]$</p>
</li>
<li>
<p><strong>注意</strong>：不需要存 $Q$，因为 $Q$ 只代表“当下的查询需求”，用完即废。</p>
</li>
</ul>
</li>
<li>
<p>预测下一个词：</p>
<p>利用注意力机制（Masked Attention，$q_3$ 只能看 $k_1, k_2, k_3$），算出 $x_3$ (“is”) 位置的输出向量 $\mathbf{z}_3$。</p>
<p>$\mathbf{z}_3$ 经过线性层 -&gt; Softmax -&gt; 采样 -&gt; 得到 token “on” ($x_4$)。</p>
</li>
</ol>
<hr>
<h3 id="第二阶段：Decode（解码-逐词生成）">第二阶段：Decode（解码 / 逐词生成）</h3>
<p>现在输入变成了 <code>[&quot;The&quot;, &quot;cat&quot;, &quot;is&quot;, &quot;on&quot;]</code>。我们需要预测下一个词。</p>
<h4 id="1-笨办法-Without-KV-Cache">1. 笨办法 (Without KV Cache)</h4>
<p>把这 4 个词重新喂给模型。模型会重新计算 “The”, “cat”, “is” 的 $K$ 和 $V$。</p>
<ul>
<li><strong>浪费</strong>：前 3 个词的 $K, V$ 和上一步算的一模一样，完全是重复劳动。随着句子变长（比如 10k token），99.9% 的算力都在做重复计算。</li>
</ul>
<h4 id="2-KV-Cache-办法-With-KV-Cache">2. KV Cache 办法 (With KV Cache)</h4>
<p>我们<strong>只把新生成的 token</strong> $x_4$ (“on”) 喂进模型。</p>
<p>Step 1: 仅计算新 Token 的映射</p>
<p>只计算 “on” 的向量：</p>
<p>$$q_4 = x_4 W^Q, \quad k_4 = x_4 W^K, \quad v_4 = x_4 W^V$$</p>
<ul>
<li>维度：都是 $(1 \times d_k)$ 的向量，计算量极小。</li>
</ul>
<p>Step 2: 拼接缓存 (Append to Cache)</p>
<p>把新的 $k_4, v_4$ 拼接到之前的 Cache 屁股后面：</p>
<ul>
<li>
<p><code>Cache_K</code> (新) = Concat($[k_1, k_2, k_3]$, $k_4$) $\rightarrow$ 维度变成 $(4 \times d_k)$</p>
</li>
<li>
<p><code>Cache_V</code> (新) = Concat($[v_1, v_2, v_3]$, $v_4$) $\rightarrow$ 维度变成 $(4 \times d_k)$</p>
</li>
</ul>
<p>Step 3: 注意力计算 (Attention Step)</p>
<p>这是最精妙的一步。</p>
<p>Query 只有 1 个 ($q_4$)，但 Key 有 4 个 (历史 + 现在)。</p>
<p>$$\text{Attention Score} = \text{softmax}\left( \frac{q_4 \cdot \text{Cache_K}^T}{\sqrt{d_k}} \right)$$</p>
<ul>
<li>
<p>数学运算：$(1 \times d_k) \times (d_k \times 4) \rightarrow (1 \times 4)$。</p>
</li>
<li>
<p>含义：$q_4$ (“on”) 回头看了一眼历史 (“The”, “cat”, “is”, “on”)，计算出它和这 4 个词的相关性。</p>
</li>
</ul>
<p>Step 4: 聚合 (Aggregation)</p>
<p>$$\mathbf{z}_4 = \text{Score} \cdot \text{Cache_V}$$</p>
<ul>
<li>数学运算：$(1 \times 4) \times (4 \times d_v) \rightarrow (1 \times d_v)$。</li>
</ul>
<p>Step 5: 输出</p>
<p>$\mathbf{z}_4$ 经过后续层，预测出 “the” ($x_5$)。</p>
<hr>
<h3 id="循环往复-The-Loop">循环往复 (The Loop)</h3>
<p>现在有了 “the” ($x_5$)：</p>
<ol>
<li>
<p>输入仅为 $x_5$。</p>
</li>
<li>
<p>算 $q_5, k_5, v_5$。</p>
</li>
<li>
<p>Cache 变长：$[k_1…k_4] + k_5$。</p>
</li>
<li>
<p>$q_5$ 对 5 个 Key 做 Attention。</p>
</li>
<li>
<p>输出 $x_6$。</p>
</li>
</ol>
<p>直到模型输出一个特殊的 <strong>[EOS] (End of Sequence)</strong> token，循环结束。</p>
<hr>
<h3 id="矩阵视角的-KV-Cache-动态演变">矩阵视角的 KV Cache 动态演变</h3>
<p>让我们用矩阵来看看显存里发生了什么。假设 $d_k=64$，我们在第 $t$ 步生成。</p>
<table>
<thead>
<tr>
<th><strong>步骤</strong></th>
<th><strong>输入 (Input)</strong></th>
<th><strong>Query 维度</strong></th>
<th><strong>KV Cache 维度 (显存中)</strong></th>
<th><strong>计算复杂度 (Attention)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prefill</strong> ($t=3$)</td>
<td>“The cat is”</td>
<td>$(3 \times 64)$</td>
<td>$(3 \times 64)$</td>
<td>$O(3^2)$</td>
</tr>
<tr>
<td><strong>Decode</strong> ($t=4$)</td>
<td>“on”</td>
<td>$(1 \times 64)$</td>
<td>$(4 \times 64)$</td>
<td>$O(1 \times 4)$</td>
</tr>
<tr>
<td><strong>Decode</strong> ($t=5$)</td>
<td>“the”</td>
<td>$(1 \times 64)$</td>
<td>$(5 \times 64)$</td>
<td>$O(1 \times 5)$</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td><strong>Decode</strong> ($t=1000$)</td>
<td>“…”</td>
<td>$(1 \times 64)$</td>
<td>$(1000 \times 64)$</td>
<td>$O(1 \times 1000)$</td>
</tr>
</tbody>
</table>
<p><strong>关键观察</strong>：</p>
<ol>
<li>
<p><strong>显存杀手</strong>：KV Cache 的大小是随序列长度<strong>线性增长</strong>的。</p>
<ul>
<li>这就是为什么长文本（Long Context）非常吃显存。一个 7B 模型，如果上下文开到 128k，KV Cache 占用的显存可能比模型本身参数还要大！</li>
</ul>
</li>
<li>
<p><strong>IO 瓶颈</strong>：在 Decode 阶段，每次计算只处理 1 个 token，但需要把巨大的 KV Cache 矩阵从显存（HBM）搬运到计算单元（SRAM）。</p>
<ul>
<li>这就是为什么推理时 GPU 核心经常在“偷懒”，因为都在等数据搬运。这也催生了 <strong>FlashAttention</strong> 等技术来优化读写。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="总结：为什么不缓存-Q？">总结：为什么不缓存 Q？</h3>
<p>这是一个高频面试题。</p>
<ul>
<li>
<p><strong>Q (Query)</strong> 是“观察者”。在第 $t$ 步，我们只需要基于<strong>当前</strong> token 的视角去观察过去。过去的 Query ($q_1, q_2…$) 是当时的观察者，它们的任务已经完成了（输出了当时的 next token），对现在没有帮助。</p>
</li>
<li>
<p><strong>K, V (Key, Value)</strong> 是“被观察的资料”。过去的资料（“The”, “cat”…）一直存在那里，随时准备被未来的 token ($q_{100}, q_{101}…$) 查询和引用，所以必须缓存。</p>
</li>
</ul>
<p>训练（Training）和推理（Inference）最大的区别在于：<strong>推理是唯唯诺诺的“走一步看一步”，而训练是开启上帝视角的“并行轰炸”。</strong></p>
<p>在训练时，我们已经拥有了整个句子的标准答案（Ground Truth）。我们不需要像推理那样，等着产出 “The” 才能去算 “cat”。我们可以<strong>一次性</strong>把所有词都喂进去，同时计算所有位置的预测误差。</p>
<p>这一机制的核心叫做 <strong>Teacher Forcing（教师强制）</strong>，配合 <strong>Causal Masking（因果掩码）</strong> 来实现。</p>
<p>我们把流程拆解开，看看数学和矩阵上发生了什么变化。</p>
<hr>
<h3 id="1-核心差异：Teacher-Forcing-教师强制">1. 核心差异：Teacher Forcing (教师强制)</h3>
<p>假设我们要训练句子：<code>&quot;The cat is on&quot;</code>。</p>
<ul>
<li>
<p>推理时 (Inference)：</p>
<p>模型预测 “The” -&gt; 模型产出 “dog” (错了) -&gt; 下一步只能基于 “dog” 去预测 -&gt; 错上加错 (Error Accumulation)。</p>
<p>这是一场没有回头的接力赛。</p>
</li>
<li>
<p>训练时 (Training)：</p>
<p>不管模型在第 1 步预测的是 “dog” 还是 “cat”，我们在第 2 步强制喂给它的输入永远是正确的历史 “The”。</p>
<p>我们将整个正确序列错位后作为监督信号：</p>
<ul>
<li>
<p><strong>Input</strong>: <code>[&quot;&lt;BOS&gt;&quot;, &quot;The&quot;, &quot;cat&quot;, &quot;is&quot;]</code></p>
</li>
<li>
<p><strong>Label</strong>: <code>[&quot;The&quot;, &quot;cat&quot;, &quot;is&quot;, &quot;on&quot;]</code></p>
</li>
</ul>
</li>
</ul>
<p>这意味着：模型在计算 “is” (Input) -&gt; “on” (Label) 的概率时，完全不需要等待 “The” -&gt; “cat” 这一步算完。它可以<strong>同时</strong>计算。</p>
<hr>
<h3 id="2-数学魔法：因果掩码-Causal-Masking">2. 数学魔法：因果掩码 (Causal Masking)</h3>
<p>你可能会问：<em>“如果把整个句子一起输进去，计算 ‘The’ 的时候，模型岂不是能偷看到后面的 ‘cat’ 和 ‘is’？”</em></p>
<p>这就是 Transformer 训练时最核心的矩阵操作：<strong>Masking</strong>。</p>
<p>在计算 Attention Score ($QK^T$) 时，我们会加上一个<strong>下三角掩码矩阵 (Lower Triangular Mask Matrix)</strong>$\mathbf{M}$。</p>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T + \mathbf{M}}{\sqrt{d_k}} \right) V$$</p>
<p>其中 $\mathbf{M}$ 的定义是：</p>
<p>$$M_{ij} = \begin{cases} 0 &amp; \text{if } i \ge j \text{ (能看)} \ -\infty &amp; \text{if } i &lt; j \text{ (不能看，偷看未来)} \end{cases}$$</p>
<h4 id="矩阵可视化">矩阵可视化</h4>
<p>假设序列长度为 4，Attention Score 矩阵（$4 \times 4$）在 Softmax 之前会被加上这样一个矩阵：</p>
<p>$$\begin{bmatrix} 0 &amp; -\infty &amp; -\infty &amp; -\infty \ 0 &amp; 0 &amp; -\infty &amp; -\infty \ 0 &amp; 0 &amp; 0 &amp; -\infty \ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}$$</p>
<ul>
<li>
<p><strong>第 1 行 (对应第 1 个词)</strong>：只能看第 1 列 (自己)，后面全是 $-\infty$。</p>
</li>
<li>
<p><strong>第 2 行 (对应第 2 个词)</strong>：能看第 1、2 列 (过去和自己)，后面是 $-\infty$。</p>
</li>
<li>
<p><strong>Softmax 之后</strong>：$-\infty$ 变成了 $0$。也就是说，<strong>对于第 $i$ 个词，它对未来词的注意力权重被强制归零。</strong></p>
</li>
</ul>
<p>通过这个矩阵操作，我们在并行计算的同时，在逻辑上模拟了“时间只能向前流逝”。</p>
<hr>
<h3 id="3-完整的训练-Step-计算流">3. 完整的训练 Step 计算流</h3>
<p><strong>Step 1: 准备数据</strong></p>
<ul>
<li>
<p>输入矩阵 $\mathbf{X} \in \mathbb{R}^{n \times d_{model}}$ (包含整个句子)。</p>
</li>
<li>
<p>目标索引 $\mathbf{Y} \in \mathbb{R}^n$ (向左移一位的句子)。</p>
</li>
</ul>
<p><strong>Step 2: 前向传播 (Forward Pass) - 并行！</strong></p>
<ul>
<li>
<p>计算 $Q, K, V$ (一次矩阵乘法)。</p>
</li>
<li>
<p>计算 $QK^T$ (一次矩阵乘法)。</p>
</li>
<li>
<p><strong>应用 Mask</strong> (加上 $-\infty$ 三角阵)。</p>
</li>
<li>
<p>Softmax + 聚合 $V$。</p>
</li>
<li>
<p>经过 FFN 和层归一化。</p>
</li>
<li>
<p>输出预测 logits $\mathbf{Z} \in \mathbb{R}^{n \times V_{vocab}}$ (这里 $n$ 是序列长，$V_{vocab}$ 是词表大小)。</p>
</li>
</ul>
<p>Step 3: 计算 Loss (Parallel Loss Computation)</p>
<p>我们不需要循环。我们拿到的是 $n$ 个位置的预测分布。</p>
<p>直接计算交叉熵损失 (Cross Entropy Loss)：</p>
<p>$$\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \text{CrossEntropy}(\mathbf{Z}_i, \mathbf{Y}_i)$$</p>
<p>这实际上是在说：</p>
<ul>
<li>
<p>第 1 个位置：预测 “The” 的概率够大吗？</p>
</li>
<li>
<p>第 2 个位置：预测 “cat” 的概率够大吗？</p>
</li>
<li>
<p>…</p>
</li>
<li>
<p><strong>同时</strong>把这些误差加起来求平均。</p>
</li>
</ul>
<p><strong>Step 4: 反向传播 (Backward Pass)</strong></p>
<ul>
<li>根据总 Loss $\mathcal{L}$，一次性计算所有参数的梯度，更新权重。</li>
</ul>
<hr>
<h3 id="4-训练-vs-推理：残酷的对比表">4. 训练 vs 推理：残酷的对比表</h3>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>Training (训练)</strong></th>
<th><strong>Inference (推理)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>并行性</strong></td>
<td><strong>高度并行 (Highly Parallel)</strong><br><br>  <br><br>整个句子一次进，$O(1)$ 时间复杂度（层数层面）。</td>
<td><strong>串行 (Sequential)</strong><br><br>  <br><br>一个词一个词出，Loop $n$ 次。</td>
</tr>
<tr>
<td><strong>KV Cache</strong></td>
<td><strong>不需要</strong><br><br>  <br><br>因为一次性全算完了，不需要缓存上一步。但需要保存<strong>Activations</strong>（中间激活值）用于反向传播，这比 KV Cache 还要大得多。</td>
<td><strong>必须</strong><br><br>  <br><br>否则计算量爆炸。</td>
</tr>
<tr>
<td><strong>输入数据</strong></td>
<td>Ground Truth (正确答案)。</td>
<td>模型上一步生成的输出 (可能是错的)。</td>
</tr>
<tr>
<td><strong>Masking</strong></td>
<td><strong>显式掩码 (Explicit Masking)</strong><br><br>  <br><br>用 $-\infty$ 矩阵盖住未来。</td>
<td><strong>隐式因果 (Implicit Causality)</strong><br><br>  <br><br>物理上就没有未来数据，不需要 Mask 未来（但可能需要 Mask Pad token）。</td>
</tr>
<tr>
<td><strong>显存瓶颈</strong></td>
<td><strong>Activations (激活值)</strong><br><br>  <br><br>存每一层的中间结果供反向传播。Batch Size 大时显存爆炸。</td>
<td><strong>KV Cache</strong><br><br>  <br><br>存 KV 供下一步 Attention。Context 长时显存爆炸。</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="5-为什么训练不用-KV-Cache？">5. 为什么训练不用 KV Cache？</h3>
<p>这是一个非常深入的问题。</p>
<ul>
<li>
<p><strong>KV Cache 的目的</strong>是：避免重复计算历史 token 的 $K$ 和 $V$。</p>
</li>
<li>
<p><strong>在训练时</strong>：我们把所有 token 放在一个大矩阵里一次性乘完了。$K$ 矩阵本身就是一次性算出来的完整矩阵。我们<strong>没有</strong>“重复计算”的问题，因为我们没有循环。</p>
</li>
<li>
<p>所以，训练时不需要 KV Cache 这种机制。相反，训练需要的是 <strong>Checkpointing (梯度检查点)</strong>，这是为了省显存，不存某些层的 Activation，等反向传播时重算，但这属于显存优化的范畴了。</p>
</li>
</ul>

                </div>
                

                
                <div class="mdui-divider"></div>
                <div class="mdui-card-actions mdui-p-x-3 mdui-p-y-2">
                    <div class="mdui-row">
                        <div class="mdui-col-sm-3 mdui-text-left">
                            
                                <a href="/Anti-Sycophancy%20Prompts/" class="mdui-ripple mdui-hoverable mdui-p-a-1 mdui-text-truncate" style="display: block; text-decoration: none;">
                                    <div class="mdui-typo-caption mdui-text-color-theme-accent mdui-text-truncate">
                                        <i class="mdui-icon material-icons" style="font-size: 14px; vertical-align: middle;">arrow_back</i> 
                                        Previous
                                    </div>
                                    <div class="mdui-typo-body-1 mdui-m-t-1 mdui-text-truncate mdui-text-color-black" style="font-weight: 500;">
                                        笔记 | Anti-Sycophancy Prompts
                                    </div>
                                </a>
                            
                        </div>
                        <div class="mdui-col-sm-6"></div>
                        <div class="mdui-col-sm-3 mdui-text-right">
                            
                                <a href="/%E5%90%8E%E8%AE%AD%E7%BB%83/" class="mdui-ripple mdui-hoverable mdui-p-a-1 mdui-text-truncate" style="display: block; text-decoration: none;">
                                    <div class="mdui-typo-caption mdui-text-color-theme-accent mdui-text-truncate">
                                        Next
                                        <i class="mdui-icon material-icons" style="font-size: 14px; vertical-align: middle;">arrow_forward</i>
                                    </div>
                                    <div class="mdui-typo-body-1 mdui-m-t-1 mdui-text-truncate mdui-text-color-black" style="font-weight: 500;">
                                        笔记 | 后训练
                                    </div>
                                </a>
                            
                        </div>
                    </div>
                </div>
                
                
            </div>
            
        </div>
        <!-- 侧边栏 -->
        <div class="mdui-col-md-3 mdui-col-xs-12  mdui-p-t-1 mdui-p-b-4 sticky-sidebar" id="post-sidebar">
            
                <div class="mdui-card mdui-shadow-1 toc-full-height">
                    <div class="mdui-card-content toc-card-content">
                        <div class="mdui-typo-heading mdui-m-b-2">Table of Contents</div>
                        <div class="toc-content">
                            <ol class="mdui-list mdui-typo"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E5%AE%9A%E4%B9%89%E7%A9%BA%E9%97%B4%E4%B8%8E%E8%BE%93%E5%85%A5"><span class="mdui-list mdui-typo-text">1. 定义空间与输入</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E6%98%A0%E5%B0%84%E5%88%B0%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4-Projections"><span class="mdui-list mdui-typo-text">2. 映射到特征空间 (Projections)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E5%AE%9A%E4%B9%89%E7%9B%B8%E5%85%B3%E6%80%A7%E5%BA%A6%E9%87%8F-Aggregation-Weights"><span class="mdui-list mdui-typo-text">3. 定义相关性度量 (Aggregation Weights)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0-The-Aggregation"><span class="mdui-list mdui-typo-text">4. 聚合函数 (The Aggregation)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#5-%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A%EF%BC%9A%E5%87%B8%E7%BB%84%E5%90%88-Convex-Combination"><span class="mdui-list mdui-typo-text">5. 几何解释：凸组合 (Convex Combination)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#6-%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F-Global-Aggregation"><span class="mdui-list mdui-typo-text">6. 矩阵形式 (Global Aggregation)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E5%AE%83%E4%B8%8E%E4%BC%A0%E7%BB%9F%E8%81%9A%E5%90%88%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="mdui-list mdui-typo-text">总结：它与传统聚合的区别</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%EF%BC%9A%E4%BB%8E%E2%80%9C%E5%8D%95%E6%AC%A1%E8%81%9A%E5%90%88%E2%80%9D%E5%88%B0%E2%80%9C%E5%B9%B6%E8%A1%8C%E8%81%9A%E5%90%88%E2%80%9D"><span class="mdui-list mdui-typo-text">1. 直观理解：从“单次聚合”到“并行聚合”</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E8%AF%A6%E7%BB%86%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC%EF%BC%88%E5%B8%A6%E7%BB%B4%E5%BA%A6%E8%BF%BD%E8%B8%AA%EF%BC%89"><span class="mdui-list mdui-typo-text">2. 详细数学推导（带维度追踪）</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E5%AE%8C%E6%95%B4%E7%9A%84%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E6%80%BB%E7%BB%93"><span class="mdui-list mdui-typo-text">3. 完整的矩阵形式总结</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E5%87%A0%E4%BD%95%E8%A7%86%E8%A7%92%E7%9A%84%E4%BF%AE%E6%AD%A3"><span class="mdui-list mdui-typo-text">4. 几何视角的修正</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E9%80%9A%E7%94%A8%E8%81%9A%E5%90%88%E8%8C%83%E5%BC%8F-The-General-Aggregation-Framework"><span class="mdui-list mdui-typo-text">通用聚合范式 (The General Aggregation Framework)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN-%E5%B1%80%E9%83%A8%E3%80%81%E9%9D%99%E6%80%81%E3%80%81%E4%BD%8D%E7%BD%AE%E4%BE%9D%E8%B5%96"><span class="mdui-list mdui-typo-text">1. 卷积神经网络 (CNN): 局部、静态、位置依赖</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-RNN-%E6%97%B6%E5%BA%8F%E3%80%81%E9%80%92%E5%BD%92%E3%80%81%E5%8E%86%E5%8F%B2%E4%BE%9D%E8%B5%96"><span class="mdui-list mdui-typo-text">2. 循环神经网络 (RNN): 时序、递归、历史依赖</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B-Self-Attention-%E5%85%A8%E5%B1%80%E3%80%81%E5%8A%A8%E6%80%81%E3%80%81%E5%86%85%E5%AE%B9%E4%BE%9D%E8%B5%96"><span class="mdui-list mdui-typo-text">3. 自注意力 (Self-Attention): 全局、动态、内容依赖</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E7%BB%88%E6%9E%81%E5%AF%B9%E6%AF%94%E8%A1%A8-The-Unified-Comparison-Matrix"><span class="mdui-list mdui-typo-text">4. 终极对比表 (The Unified Comparison Matrix)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#5-%E6%80%BB%E7%BB%93"><span class="mdui-list mdui-typo-text">5. 总结</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E6%A0%B8%E5%BF%83%E7%BB%B4%E5%BA%A6%E7%9A%84%E2%80%9C%E9%BB%84%E9%87%91%E6%B3%95%E5%88%99%E2%80%9D"><span class="mdui-list mdui-typo-text">1. 核心维度的“黄金法则”</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E5%85%B8%E5%9E%8B%E9%87%8F%E7%BA%A7%E9%85%8D%E7%BD%AE%E8%A1%A8-The-Standard-Tiers"><span class="mdui-list mdui-typo-text">2. 典型量级配置表 (The Standard Tiers)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E7%8E%B0%E4%BB%A3%E6%9E%B6%E6%9E%84%E7%9A%84%E7%89%B9%E6%AE%8A%E5%8F%98%E5%8A%A8-Modern-Innovations"><span class="mdui-list mdui-typo-text">3. 现代架构的特殊变动 (Modern Innovations)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E6%80%BB%E7%BB%93%EF%BC%9A%E5%A6%82%E6%9E%9C%E8%AE%A9%E4%BD%A0%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="mdui-list mdui-typo-text">4. 总结：如果让你设计一个模型</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9APrefill%EF%BC%88%E9%A2%84%E5%A1%AB%E5%85%85-%E9%A6%96%E8%AF%8D%E5%A4%84%E7%90%86%EF%BC%89"><span class="mdui-list mdui-typo-text">第一阶段：Prefill（预填充 &#x2F; 首词处理）</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9ADecode%EF%BC%88%E8%A7%A3%E7%A0%81-%E9%80%90%E8%AF%8D%E7%94%9F%E6%88%90%EF%BC%89"><span class="mdui-list mdui-typo-text">第二阶段：Decode（解码 &#x2F; 逐词生成）</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E5%BE%AA%E7%8E%AF%E5%BE%80%E5%A4%8D-The-Loop"><span class="mdui-list mdui-typo-text">循环往复 (The Loop)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E7%9F%A9%E9%98%B5%E8%A7%86%E8%A7%92%E7%9A%84-KV-Cache-%E5%8A%A8%E6%80%81%E6%BC%94%E5%8F%98"><span class="mdui-list mdui-typo-text">矩阵视角的 KV Cache 动态演变</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%BC%93%E5%AD%98-Q%EF%BC%9F"><span class="mdui-list mdui-typo-text">总结：为什么不缓存 Q？</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E6%A0%B8%E5%BF%83%E5%B7%AE%E5%BC%82%EF%BC%9ATeacher-Forcing-%E6%95%99%E5%B8%88%E5%BC%BA%E5%88%B6"><span class="mdui-list mdui-typo-text">1. 核心差异：Teacher Forcing (教师强制)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E6%95%B0%E5%AD%A6%E9%AD%94%E6%B3%95%EF%BC%9A%E5%9B%A0%E6%9E%9C%E6%8E%A9%E7%A0%81-Causal-Masking"><span class="mdui-list mdui-typo-text">2. 数学魔法：因果掩码 (Causal Masking)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E5%AE%8C%E6%95%B4%E7%9A%84%E8%AE%AD%E7%BB%83-Step-%E8%AE%A1%E7%AE%97%E6%B5%81"><span class="mdui-list mdui-typo-text">3. 完整的训练 Step 计算流</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E8%AE%AD%E7%BB%83-vs-%E6%8E%A8%E7%90%86%EF%BC%9A%E6%AE%8B%E9%85%B7%E7%9A%84%E5%AF%B9%E6%AF%94%E8%A1%A8"><span class="mdui-list mdui-typo-text">4. 训练 vs 推理：残酷的对比表</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#5-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%AD%E7%BB%83%E4%B8%8D%E7%94%A8-KV-Cache%EF%BC%9F"><span class="mdui-list mdui-typo-text">5. 为什么训练不用 KV Cache？</span></a></li></ol>
                        </div>
                    </div>
                </div>
            
            <div class="mdui-m-t-2">
                <div class="mdui-card mdui-shadow-1">
  <div class="mdui-card-media" style="height: 120px; overflow: hidden;">
    <img src="/image/card.png" class="mdui-img-fluid" style="width: 100%; height: 100%; object-fit: cover;">
  </div>

  <div class="mdui-card-content mdui-text-center" style="position: relative;">
    
      <img src="/image/avatar.jpeg" class="mdui-img-circle mdui-shadow-4 sidebar-avatar-offset" style="width: 80px; height: 80px; border: 2px solid white; display: block; margin-left: auto; margin-right: auto;">
    

    <div class="mdui-typo-headline mdui-m-t-2">NaN / Not a Name</div>
    <div class="mdui-typo-caption mdui-text-color-grey-600 mdui-m-t-1">
      Nomen a Nullo. Nihil ad Nomen.
    </div>
  </div>

  <!-- <div class="mdui-divider"></div>

  <div class="mdui-row mdui-text-center mdui-p-y-2">
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">83</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">posts</div>
    </div>
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">5</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">Categories</div>
    </div>
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">27</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">Tags</div>
    </div>
  </div>

  <div class="mdui-divider"></div> -->

  <div class="mdui-card-actions mdui-text-center">
    
        
            <a href="mailto:your@email.com" target="_blank" class="mdui-btn mdui-btn-icon mdui-ripple mdui-text-color-blue" mdui-tooltip="{content: 'Email'}">
                <i class="mdui-icon material-icons">email</i>
            </a>
        
            <a href="https://github.com/yourusername" target="_blank" class="mdui-btn mdui-btn-icon mdui-ripple mdui-text-color-black" mdui-tooltip="{content: 'Github'}">
                <i class="mdui-icon material-icons">code</i>
            </a>
        
    
  </div>
</div>
            </div>
        </div>
    </div>
</div>
<div id="page-bottom" aria-hidden="true"></div>
  </div>
  <!-- Footer -->
  <footer class="mdui-color-grey-200 mdui-text-center mdui-typo-caption">
  <div class="mdui-container">
    <div class="mdui-p-y-2">
      &copy; 2026 <span class="mdui-text-color-theme">NaN / Not a Name</span>
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io/" class="mdui-text-color-theme-accent">Hexo</a> | <a target="_blank" rel="noopener" href="https://www.mdui.org/" class="mdui-text-color-theme-accent">MDUI v1</a>
    </div>
  </div>
</footer>

  <!-- Scripts -->
  <script src="/mdui/js/mdui.min.js"></script>
  <script src="/katex/katex.min.js"></script>
  <script src="/katex/contrib/auto-render.min.js"></script>
  
  <script>
    // 合并所有 DOMContentLoaded 监听器
    document.addEventListener("DOMContentLoaded", function() {
      // KaTeX 数学公式渲染
      renderMathInElement(document.body, {
        // 自定义分隔符，支持 $...$ 和 $$...$$
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ],
        // 忽略的标签
        ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
      });

      // 自动为所有表格和图片添加 MDUI 样式类
        var tables = document.getElementsByTagName("table");
        for (var i = 0; i < tables.length; i++) {
            if (!tables[i].classList.contains("mdui-table")) {
                tables[i].classList.add("mdui-table", "mdui-shadow-0");
            }
        }
        var imgs = document.getElementsByTagName("img");
        for (var i = 0; i < imgs.length; i++) {
            if (!imgs[i].classList.contains("mdui-img-fluid")) {
                imgs[i].classList.add("mdui-img-fluid");
            }
        }
    });
  </script>
</body>
</html>