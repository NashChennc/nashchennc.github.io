<!DOCTYPE html>
<html lang="cn">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>强化学习 - NaN&#39;s Blog ｜ Not a Nihilist</title>

  <!-- MDUI CSS -->
  <link rel="stylesheet" href="/mdui/css/mdui.min.css">

  <!-- KaTeX CSS (If needed) -->
  <link rel="stylesheet" href="/katex/katex.min.css">

  <!-- Custom CSS (最小化，只包含无法用 MDUI 实现的部分) -->
  <link rel="stylesheet" href="/css/custom.css">

  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <meta name="theme-color" content="#3F51B5">
<meta name="generator" content="Hexo 8.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body class="mdui-appbar-with-toolbar mdui-theme-primary-indigo mdui-theme-accent-pink mdui-color-grey-100">
  
  <!-- Header/AppBar -->
  <header class="appbar mdui-appbar mdui-appbar-fixed">
    <div class="mdui-toolbar mdui-color-theme">
        
          <div class="mdui-toolbar-spacer"></div>
            <div class="mdui-tab mdui-tab-full-width">
                
                <a href="/" class="mdui-ripple ">
                    Home
                </a>
                
                
                    
                        <a href="/categories/笔记" class="mdui-ripple ">
                            笔记 | 6
                        </a>
                    
                
                    
                        <a href="/categories/科研" class="mdui-ripple ">
                            科研 | 7
                        </a>
                    
                
                    
                        <a href="/categories/碎碎念" class="mdui-ripple ">
                            碎碎念 | 29
                        </a>
                    
                
                    
                        <a href="/categories/流水账" class="mdui-ripple ">
                            流水账 | 4
                        </a>
                    
                
                    
                        <a href="/categories/工具箱" class="mdui-ripple ">
                            工具箱 | 16
                        </a>
                    
                
                    
                        <a href="/categories/其它" class="mdui-ripple ">
                            其它 | 4
                        </a>
                    
                
            </div>
        
        <div class="mdui-toolbar-spacer"></div>
        <a href="/about" class="mdui-btn mdui-btn-icon mdui-ripple" mdui-tooltip="{content: 'About'}">
            <i class="mdui-icon material-icons"> more_vert </i>
        </a>
    </div>
</header>

  <!-- Main Content (Layout control moved to individual views) -->
  <div id="main-content">
    <!-- 全宽 Banner -->
<div class="banner-container" style="height: 350px;">
    <div class="banner-bg" style="background-image: url(/image/banner.png);"></div>
    
    <div class="mdui-card-media-covered banner-overlay" style="background: rgba(0, 0, 0, 0.4);">
        <div class="mdui-container">
            <div class="mdui-card-primary mdui-float-right mdui-text-color-white mdui-p-y-2">
                <div class="mdui-card-primary-title" style="font-weight: 300;">
                    NaN&#39;s Blog ｜ Not a Nihilist
                </div>
                <div class="mdui-card-primary-subtitle mdui-float-right" style="opacity: 0.9;">
                    Nomen a Nullo. Nihil ad Nomen.
                </div>
            </div>
        </div>
    </div>
</div>
<!-- FAB -->
<div id="exampleFab" class="mdui-fab-wrapper" mdui-fab="{trigger: 'hover'}">
    <a class="mdui-fab mdui-ripple mdui-color-theme-accent" href="/archives" mdui-tooltip="{content: 'goto Archive', position: 'left'}">
        <i class="mdui-icon material-icons">bookmark</i>
        <i class="mdui-icon mdui-fab-opened material-icons">bookmark</i>
    </a>
    <div class="mdui-fab-dial">
        <a href="/about" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-teal">
            <i class="mdui-icon material-icons">more_vert</i>
        </a>
        <a href="#top" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-blue-700" mdui-tooltip="{content: 'goto top', position: 'left'}">
            <i class="mdui-icon material-icons">keyboard_arrow_up</i>
        </a>
        <a href="#page-bottom" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-indigo" mdui-tooltip="{content: 'goto bottom', position: 'left'}">
            <i class="mdui-icon material-icons">keyboard_arrow_down</i>
        </a>
    </div>
</div>

<div class="mdui-container">
    <div class="mdui-row mdui-p-t-1 mdui-p-b-1">
        <!-- 文章内容 -->

        <div class="mdui-col-md-9 mdui-col-xs-12 mdui-p-t-1 mdui-p-b-4">
            <div class="mdui-card mdui-hoverable">
                <div class="mdui-card-primary mdui-p-x-5">
                    <div class="mdui-card-primary-title mdui-typo-display-1">
                        强化学习
                    </div>
                    <div class="mdui-row mdui-p-t-2">
                        <div class="mdui-col-xs-9 mdui-typo-caption-opacity">
                            Last Update: 2025/12/30 17:19
                        </div>
                        <div class="mdui-col-xs-3">
                            <div class="mdui-float-right mdui-typo">
                                
                                    <a href="/categories/%E7%AC%94%E8%AE%B0/">
                                        笔记
                                    </a>
                                
                            </div>
                        </div>
                    </div>
                    <div class="mdui-divider"></div>
                    <br/>
                </div>
                <div class="mdui-card-content mdui-typo mdui-p-x-5">
                    <h2 id="0-强化学习的定义">0. 强化学习的定义</h2>
<table>
<thead>
<tr>
<th><strong>符号 (Symbol)</strong></th>
<th><strong>含义 (Meaning)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>$s\in \mathcal S$</td>
<td><strong>状态 (State)</strong> 。环境的客观情况。</td>
</tr>
<tr>
<td>$a\in \mathcal A$</td>
<td><strong>动作 (Action)</strong> 。可供智能体选择执行的一个动作。</td>
</tr>
<tr>
<td>$r\in \mathbb R$</td>
<td><strong>反馈 (Reward)</strong> 。由环境反馈，定义模型最终目标的一个标量数值。</td>
</tr>
<tr>
<td>$p(s’, r \mid s, a):\mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0, 1]$</td>
<td><strong>环境动力学 (Dynamics)</strong>。在状态 $s$ 下，智能体执行了动作 $a$ 的情况下，环境转移到 $s’$ 并产生反馈 $r$ 的联合概率分布。</td>
</tr>
<tr>
<td>$Q(s, a):\mathcal S\times\mathcal A→\mathbb R$</td>
<td><strong>动作价值函数</strong> 。<strong>智能体</strong> 对于在状态 $s$ 下选择动作 $a$ 的长期期望回报估计。</td>
</tr>
<tr>
<td>$\pi(a\mid s):\mathcal S\times\mathcal A→[0,1]$</td>
<td><strong>策略 (Policy)</strong> 。<strong>智能体</strong> 对于在状态 $s$ 下选择动作 $a$ 给出的的概率分布。</td>
</tr>
</tbody>
</table>
<ul>
<li>在强化学习中，首先会存在一个初始环境 $S_0$ 。</li>
<li>当智能体在 $t$ 时刻处于状态 $S_t$ 时，它会根据 $\pi$ 选择一个下一步执行的动作。
$$A_t\sim\pi(\cdot\mid s)$$</li>
<li>当智能体在 $t$ 时刻处于状态 $S_t$ 并执行动作 $A_t$ 时，环境根据概率分布 $p$ 采样产生下一时刻的状态 $S_{t+1}$ 和即时奖励 $R_{t+1}$。
$$(S_{t+1}, R_{t+1}) \sim p(\cdot, \cdot \mid S_t, A_t)$$
这样，直至一回合 (Episode) 游戏结束，智能体与环境的交互会产生一个状态序列 $S_0, A_0, R_1, S_1, A_1, \dots, S_T, R_T$。</li>
</ul>
<p>所谓强化学习，就是在给定环境 $S$ 、对应的可选动作 $A(S)$ 、以及环境的演变规律 $p(s’, r \mid s, a)$ 的情况下，通过设计方法建立模型用于执行 $\pi(A|S)$ 的内部表征 $Q(s,a)$ ，从而最大化最终反馈 $R$ 的一个过程。其中，前四项构成了经典的 MDP 过程四要素 $(s,a,p,r)$ ，RL 在不显式获取这四要素的情况下，最终获得 MDP 的一个解 $\pi(\cdot\mid Q)$。</p>
<p>在早期的强化学习中，$Q(s,a)$ 由一个二维标量表格存储。此时 $\pi(A\mid S)$ 通常可以使用 $\mathrm{Softmax}_{a\in A}(Q(s,a))$ ，也就是贪心。在 DQN 中这是由一个神经网络模拟的函数。</p>
<h2 id="1-蒙特卡洛方法-Monte-Carlo-Methods">1. 蒙特卡洛方法 (Monte Carlo Methods)</h2>
<p>蒙特卡洛方法利用经验平均值来逼近期望值。其核心思想是：实际进行一整个回合的游戏（作为样本），为每个时刻的状态 $S_t,A_t$ 计算回合内的长期回报函数 $G_t$ ，也就是以 $G_t$ 为样本。动作价值函数 $Q$ 是长期回报样本 $G_t$ 的期望。$$Q(s, a) \approx \mathbb{E}_{\pi}[G_t \mid S_t=s, A_t=a]$$<strong>执行过程 (Episode-wise Update)</strong></p>
<ol>
<li>轨迹生成 (Rollout):
利用策略 $\pi$ 生成一完整回合 (Episode)：$S_0, A_0, R_1, S_1, A_1, \dots, S_T, R_T$。</li>
<li>逆序回溯 (Backward Evaluation):
对幕中出现的每一步 $t = T-1, T-2, \dots, 0$：
<ul>
<li>计算从 $t$ 时刻直至终止状态的累积折现回报 $G_t$。$$G_t \doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$</li>
<li>更新法则: 若采用增量式均值更新 (Incremental Implementation)，则执行：$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ G_t - Q(S_t, A_t) \right]$$</li>
</ul>
</li>
<li>重复直至 $Q(s,a)$ 收敛</li>
</ol>
<p>两种可选的模式</p>
<ul>
<li><strong>First-visit</strong>: 一局里如果你绕圈回到了同一个状态，只算第一次经过时的回报（无偏估计）。</li>
<li><strong>Every-visit</strong>: 每次经过都算（在某些情况下收敛更快）。</li>
</ul>
<h2 id="2-时序差分控制：SARSA-On-policy-TD-Control">2. 时序差分控制：SARSA (On-policy TD Control)</h2>
<p>SARSA 边走边学，并且怎么走就怎么学。如果不小心走了一步烂棋（探索），就降低对当前状态的评价。这是一种同策略 (On-policy) 算法，即将实际生成的策略直接用于更新价值。</p>
<p>在每一时间步中，使用当前行为策略 $\pi$ 的动作进行决策 $A_{t+1}$ ，并使用当前决策的 $Q(S_{t+1}, A_{t+1})$ 与上一状态 $Q(S_t, A_t)$ 之差得到更新方向。这是时序差分。</p>
<p>其名称源于更新元组 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$。</p>
<p><strong>执行过程 (Step-wise Update)</strong>
在每一个时间步 (Step) 中：</p>
<ol>
<li><strong>动作选择 (On-policy)</strong>: 观察由旧状态 $(S_t, A_t)$ 生成的奖励和新状态 $(R_{t+1},S_{t+1})$。在新状态 $S_{t+1}$ 下，<strong>依据当前策略 $\pi$</strong> 选择下一个动作 $A_{t+1}$。这个动作将被最终执行。</li>
<li>价值更新:<br>
使用当前决策的 $Q(S_{t+1}, A_{t+1})$ 与旧状态 $Q(S_t, A_t)$ 之差，使用五元组 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ 更新 $Q$ 值：$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ \underbrace{R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})}_{\text{TD Target}} - Q(S_t, A_t) \right]$$</li>
<li><strong>状态推进</strong>: 照常执行已选定的动作 $A_{t+1}$ ，令 $S \leftarrow S_{t+1}, A \leftarrow A_{t+1}$。</li>
</ol>
<p>对应使用估计-迭代法解一个 Bellman 方程 $$Q(S, A) = R’(S,A) + \gamma Q(S’(S,A), A’(S’\mid \pi))$$</p>
<h2 id="3-Q学习-Q-learning">3. Q学习 (Q-learning)</h2>
<p>Q-learning 是一种异策略 (Off-policy) 算法。无论行为策略如何产生数据，该算法均试图直接逼近最优动作价值函数 $Q_*$。</p>
<p><strong>执行过程 (Step-wise Update)</strong>
在每一个时间步 (Step) 中：</p>
<ol>
<li><strong>行为动作选择</strong>: 在状态 $S$ 下，根据行为策略 (如 $\epsilon$-greedy) 选择动作 $A$。</li>
<li><strong>环境响应</strong>: 执行动作 $A$，观测由环境生成的奖励 $R$ 和新状态 $S’$。</li>
<li>目标值构造 (Greedy): 不考虑下一步实际将采取的动作，而是寻找 $S’$ 下使 $Q$ 值最大的动作 $a$ 来构造目标：$$y = R + \gamma \max_{a \in \mathcal{A}} Q(S’, a)$$</li>
<li>价值更新:$$Q(S, A) \leftarrow Q(S, A) + \alpha \left[ y - Q(S, A) \right]$$</li>
<li><strong>状态推进</strong>: 令 $S \leftarrow S’$。</li>
</ol>
<h3 id="3-1-插入：在策算法与离策算法">3.1. 插入：在策算法与离策算法</h3>
<p>在悬崖行走 (Cliff Walking) 任务中，SARSA 和 Q-learning 会学出完全不同的策略。这是强化学习里最经典的对比案例之一。</p>
<h4 id="1-现象对比">1. 现象对比</h4>
<ul>
<li>
<p><strong>Q-learning (大胆派)</strong>：通常收敛得快，且发现的路径更短。它学出的路线是<strong>紧贴着悬崖边走</strong>（最优路径，步数最少）。</p>
<ul>
<li><em>后果</em>：虽然理论上是最快的，但在训练过程中，因为有 $\epsilon$ (epsilon) 的概率会随机乱走，所以它经常会“脚滑”掉进悬崖，导致训练时的平均回报波动很大。</li>
</ul>
</li>
<li>
<p><strong>SARSA (保守派)</strong>：通常收敛得稍微慢一点，且最终平均奖励（Reward）可能稍微低一点（因为它走远路）。它学出的路线通常是<strong>离悬崖有一段距离的安全路线</strong>（比如往上走一行再过去）。</p>
<ul>
<li><em>后果</em>：虽然步数多了一点（每一步 -1 分），但它极少掉进悬崖。</li>
</ul>
</li>
</ul>
<p>要从数学角度严谨地解释<strong>SARSA（On-Policy）</strong> 与 <strong>Q-learning（Off-Policy）</strong> 在风险偏好上的区别，我们需要深入分析它们各自试图逼近的<strong>贝尔曼方程（Bellman Equation）</strong> 以及在 <strong>$\epsilon$-greedy</strong> 策略下的期望值差异。</p>
<p>核心结论是：<strong>Q-learning 是风险中性（Risk-Neutral）甚至乐观的，因为它假设未来遵循最优策略；而 SARSA 是风险规避（Risk-Averse）的，因为它在价值评估中，通过实际的动作显式地计入了探索（Exploration）带来的潜在震荡或灾难性后果。</strong></p>
<p>证明详见文章附录。</p>
<h2 id="5-双重-Q-学习-Double-Q-learning">5. 双重 Q 学习 (Double Q-learning)</h2>
<p>在 Q-learning 中，目标值 $y$ 包含最大化操作 $\max_a Q(S’, a)$。若 $Q$ 值包含随机误差，该操作倾向于高估真实价值。Double Q-learning 利用两个独立的估计器解耦选择与评估过程。</p>
<p><strong>Double Q-learning</strong> 是为了解决 Q-learning 的一个通病：<strong>Maximization Bias (最大化偏差)</strong>。简单说就是 Q-learning 有时候会过度自信，高估某些动作的价值。</p>
<p><strong>执行过程 (Step-wise Update)</strong>
在每一个时间步 (Step) 中：</p>
<ol>
<li><strong>动作选择</strong>: 基于 $Q_1$ 和 $Q_2$ 的聚合 (例如 $Q_{sum} = Q_1 + Q_2$)，利用策略选择动作 $A$。</li>
<li><strong>环境响应</strong>: 执行 $A$，观测奖励 $R$ 和新状态 $S’$。</li>
<li><strong>随机更新</strong>: 以 $0.5$ 的概率决定更新 $Q_1$ (反之更新 $Q_2$)。若更新 $Q_1$：
<ul>
<li>Argmax (选择): 利用 $Q_1$ 确定在 $S’$ 下的最优动作索引：$$a^* = \operatorname*{argmax}_{a \in \mathcal{A}} Q_1(S’, a)$$</li>
<li>Evaluate (评估): 利用 $Q_2$ 估计该动作的价值：$$y = R + \gamma Q_2(S’, a^*)$$</li>
<li>更新法则:$$Q_1(S, A) \leftarrow Q_1(S, A) + \alpha [y - Q_1(S, A)]$$</li>
</ul>
</li>
<li><strong>状态推进</strong>: 令 $S \leftarrow S’$。</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center">算法名称</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">核心思想与更新公式特点</th>
<th style="text-align:center">差异与对比</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>SARSA</strong></td>
<td style="text-align:center">在线策略控制 (On-Policy Control)</td>
<td style="text-align:center"><strong>State-Action-Reward-State-Action</strong>。<br>更新 $Q(s,a)$ 时，使用<strong>实际执行</strong>的下一个动作 $a’$ 的价值。<br>公式: $y=R + \gamma Q(s’, a’)$</td>
<td style="text-align:center"><strong>保守型</strong>。因为它在更新时考虑了自己实际采取的动作（可能包含探索性质的随机动作），所以它会学习到一条相对“安全”的路径，避免探索带来的风险（如悬崖问题）。</td>
</tr>
<tr>
<td style="text-align:center"><strong>Expected SARSA</strong></td>
<td style="text-align:center">在线/离线策略控制</td>
<td style="text-align:center"><strong>期望价值更新</strong>。<br>不使用具体的 $a’$ 或 max $a’$，而是计算下一状态所有可能动作价值的<strong>期望值</strong>。<br>公式: $y=R + \gamma \sum \pi(a’\mid s’)Q(s’, a’)$</td>
<td style="text-align:center"><strong>低方差</strong>。相比SARSA，它消除了选择下一个动作带来的随机性方差，通常收敛更稳定，但计算量稍大。它可以被视为Q-Learning的一般化形式。</td>
</tr>
<tr>
<td style="text-align:center"><strong>Q-Learning</strong></td>
<td style="text-align:center">离线策略控制 (Off-Policy Control)</td>
<td style="text-align:center"><strong>贪婪策略更新</strong>。<br>更新 $Q(s,a)$ 时，不考虑实际执行的动作，而是假设下一步会采取<strong>最优</strong>动作（max）。<br>公式: $y=R + \gamma \max_{a’} Q(s’, a’)$</td>
<td style="text-align:center"><strong>激进型</strong>。它直接学习最优策略的价值（Target Policy是贪婪的），而行为策略（Behavior Policy）可以包含探索。这使得它在探索时更勇敢，但可能高估价值（Maximization Bias）。</td>
</tr>
<tr>
<td style="text-align:center"><strong>TD(0)</strong></td>
<td style="text-align:center">策略评估 (Prediction)</td>
<td style="text-align:center"><strong>基础状态价值评估</strong>。<br>只利用当前一步的奖励和下一状态的价值来更新当前状态价值 $V(s)$。</td>
<td style="text-align:center">它是TD学习的最基础形式，用于<strong>评估</strong>给定策略的价值，而不是直接用于控制（寻找最优策略）。它与MC的区别在于它使用自举（Bootstrapping）。</td>
</tr>
<tr>
<td style="text-align:center"><strong>TD($\lambda$)</strong></td>
<td style="text-align:center">多步预测 (Multi-step TD)</td>
<td style="text-align:center"><strong>引入资格迹 (Eligibility Traces)</strong>。<br>结合了TD(0)（单步）和蒙特卡洛（多步/全回合）的优势。它不仅更新当前状态，还根据$\lambda$衰减更新之前通过的所有状态。</td>
<td style="text-align:center"><strong>桥梁作用</strong>。当 $\lambda=0$ 时等同于TD(0)，当 $\lambda=1$ 时近似于蒙特卡洛。它解决了TD(0)学得慢（只传导一步）和MC方差大（受全路径随机性影响）的问题，实现了信贷分配（Credit Assignment）的平衡。</td>
</tr>
</tbody>
</table>
<h2 id="1-附录：在策算法与离策算法的区别">-1. 附录：在策算法与离策算法的区别</h2>
<h3 id="1-算法更新规则的数学形式">1. 算法更新规则的数学形式</h3>
<p>首先定义状态 $s$，动作 $a$，奖励 $r$，折扣因子 $\gamma$，学习率 $\alpha$。</p>
<h4 id="Q-learning-Off-Policy">Q-learning (Off-Policy)</h4>
<p>Q-learning 的目标是逼近最优动作价值函数 $Q^*$。其更新公式为：</p>
<p>$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a’} Q(s_{t+1}, a’) - Q(s_t, a_t) \right]$$</p>
<p>注意这里的时间差分目标（TD Target）是 $Y_{QL} = r_{t+1} + \gamma \max_{a’} Q(s_{t+1}, a’)$。</p>
<p>关键点：无论当前行为策略（Behavior Policy）如何选择下一步动作，Q-learning 在更新时都假设下一步会采取使得价值最大化的动作 $a’$。</p>
<h4 id="SARSA-On-Policy">SARSA (On-Policy)</h4>
<p>SARSA 的目标是逼近当前策略 $\pi$ 的动作价值函数 $Q^\pi$。其更新公式为：</p>
<p>$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]$$</p>
<p>这里的时间差分目标是 $Y_{SARSA} = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})$。</p>
<p>关键点：$a_{t+1}$ 是通过当前策略（通常是 $\epsilon$-greedy）实际采样得到的。</p>
<hr>
<h3 id="2-风险偏好的数学本质：期望值的展开">2. 风险偏好的数学本质：期望值的展开</h3>
<p>为了体现风险偏好，我们必须分析在收敛过程中，这两种算法如何评估一个处于“危险边缘”的状态。</p>
<p>假设当前策略 $\pi$ 为 <strong>$\epsilon$-greedy</strong> 策略，即：</p>
<ul>
<li>
<p>以 $1-\epsilon$ 的概率选择贪婪动作 $a^* = \arg\max_a Q(s, a)$。</p>
</li>
<li>
<p>以 $\epsilon$ 的概率随机选择任意动作（包括最优动作）。</p>
</li>
</ul>
<h4 id="Q-learning-的视角（最大化算子）">Q-learning 的视角（最大化算子）</h4>
<p>Q-learning 的 TD Target 的期望值（对环境转移动态 $P$ 取期望）为：</p>
<p>$$\mathbb{E}[Y_{QL} | s_t=s, a_t=a] = \sum_{s’} P(s’|s,a) \left[ R(s,a,s’) + \gamma \max_{a’} Q(s’, a’) \right]$$</p>
<p>这个公式表明，<strong>Q-learning 完全忽略了探索策略 $\epsilon$ 带来的噪声</strong>。它假设：“只要我学好了，未来我一定会走最完美的那条路”。因此，即使某个状态 $s’$ 旁边就是悬崖（负奖励极大），只要存在一条安全路径（最优动作），Q-learning 就会给 $s’$ 极高的估值。</p>
<h4 id="SARSA-的视角（期望算子）">SARSA 的视角（期望算子）</h4>
<p>SARSA 的 TD Target 的期望值不仅要对环境转移 $P$ 取期望，还要对<strong>下一步的策略 $\pi$</strong> 取期望：</p>
<p>$$\mathbb{E}[Y_{SARSA} | s_t=s, a_t=a] = \sum_{s’} P(s’|s,a) \left[ R(s,a,s’) + \gamma \sum_{a’} \pi(a’|s’) Q(s’, a’) \right]$$</p>
<p>让我们展开 $\sum_{a’} \pi(a’|s’) Q(s’, a’)$ 这一项。假设动作空间大小为 $|\mathcal{A}|$，在 $\epsilon$-greedy 策略下：</p>
<p>$$\mathbb{E}<em>{\pi}[Q(s’, \cdot)] = (1 - \epsilon) \max</em>{a’} Q(s’, a’) + \epsilon \sum_{a’} \frac{1}{|\mathcal{A}|} Q(s’, a’)$$</p>
<p><em>(注：这里采用了简化的 $\epsilon$-greedy 定义，有时 $\epsilon$ 仅分配给非贪婪动作，结论类似)</em></p>
<p>我们可以将其重写为：</p>
<p>$$\mathbb{E}<em>{\pi}[Q(s’, \cdot)] = \underbrace{\max</em>{a’} Q(s’, a’)}<em>{\text{Q-learning Term}} - \underbrace{\epsilon \left( \max</em>{a’} Q(s’, a’) - \bar{Q}(s’) \right)}_{\text{Risk Penalty (Safety Margin)}}$$</p>
<p>其中 $\bar{Q}(s’)$ 是所有动作价值的平均值。</p>
<h3 id="3-结论分析">3. 结论分析</h3>
<p>从上面的公式可以看出 SARSA 与 Q-learning 的本质区别在于那个 <strong>Risk Penalty</strong> 项：</p>
<ol>
<li>
<p><strong>风险来源</strong>：如果状态 $s’$ 处于悬崖边缘，那么它的最优动作 $Q(s’, a_{safe})$ 可能很高，但错误动作 $Q(s’, a_{cliff})$ 会极低（例如 -100）。</p>
</li>
<li>
<p><strong>Q-learning (Risk-Seeking)</strong>：只看 $\max Q$。它认为 $V(s’) \approx Q(s’, a_{safe})$。它不关心如果你手滑（$\epsilon$ 概率）掉下去会发生什么。因此，它会贴着悬崖边走最优路径。</p>
</li>
<li>
<p><strong>SARSA (Risk-Averse)</strong>：看 $\mathbb{E}[Q]$。由于 $\epsilon$ 的存在，$Q(s’, a_{cliff})$ 的极低值会拉低整体期望。</p>
<ul>
<li>
<p>数学上：$\bar{Q}(s’)$ 远小于 $\max Q(s’, a’)$。</p>
</li>
<li>
<p>结果：Risk Penalty 很大，导致 SARSA 降低对 $s’$ 的估值。</p>
</li>
<li>
<p>行为：SARSA 会认为 $s’$ 是危险的，从而倾向于选择离悬崖更远的安全路径，即使那条路稍远一点（次优）。</p>
</li>
</ul>
</li>
</ol>
<h3 id="4-总结：收敛性与应用">4. 总结：收敛性与应用</h3>
<ul>
<li>
<p><strong>Q-learning</strong> 收敛于 <strong>$Q^*$ (Optimal Value Function)</strong>。它学习的是“理论上的最优解”，假设推断时没有探索噪声。</p>
</li>
<li>
<p><strong>SARSA</strong> 收敛于 <strong>$Q^{\pi}$ (Value Function of the exploration policy)</strong>。它学习的是“在当前充满随机性的策略下的实际价值”。</p>
</li>
</ul>
<p>只要 $\epsilon &gt; 0$，SARSA 就会比 Q-learning 更保守。只有当 $\epsilon \to 0$（即 GLIE 条件满足，Greedy in the Limit with Infinite Exploration）时，SARSA 的收敛结果才会趋近于 Q-learning。</p>
<h2 id="2-最大化偏差">-2. 最大化偏差</h2>
<p>使用数学语言严谨地解释 <strong>Double Q-learning</strong> 为什么有效，我们需要聚焦于 Q-learning 的一个核心数学缺陷：<strong>最大化偏差（Maximization Bias）</strong>。</p>
<p>Double Q-learning 的核心贡献在于通过**解耦（Decoupling）**动作的选择与动作的评估，消除了标准 Q-learning 因噪声引起的高估（Overestimation）现象。</p>
<p>以下是严谨的数学推导：</p>
<hr>
<h3 id="1-问题的根源：单一估计量的最大化偏差">1. 问题的根源：单一估计量的最大化偏差</h3>
<p>在标准的 Q-learning 中，目标值（Target）的计算公式为：</p>
<p>$$Y_{Q} = r + \gamma \max_{a’} Q(s’, a’)$$</p>
<h4 id="噪声模型">噪声模型</h4>
<p>假设对于某个状态 $s’$，真实的动作价值为 $Q^*(s’, a)$。由于函数近似误差或环境随机性，我们的估计量 $Q(s’, a)$ 包含噪声：</p>
<p>$$Q(s’, a) = Q^*(s’, a) + \epsilon_a$$</p>
<p>其中 $\epsilon_a$ 是均值为 0 的随机误差变量。</p>
<h4 id="期望不等式（Jensen-不等式的变体）">期望不等式（Jensen 不等式的变体）</h4>
<p>在数学上，对于一组随机变量 $X_1, X_2, \dots, X_n$，<strong>期望的最大值不等于最大值的期望</strong>，且通常前者更小。即：</p>
<p>$$\mathbb{E} [ \max_a Q(s’, a) ] \ge \max_a \mathbb{E} [ Q(s’, a) ] = \max_a Q^*(s’, a)$$</p>
<p>直观解释：</p>
<p>假设所有动作的真实价值都为 0（$Q^*(s’, a)=0$），但由于采样噪声，有的估计值是 $0.1$，有的是 $-0.1$。</p>
<ul>
<li>
<p>$\max_a Q(s’, a)$ 会倾向于选中那个正向误差最大的动作（$0.1$）。</p>
</li>
<li>
<p>因此，更新目标 $Y_Q$ 会持续地大于真实值。</p>
</li>
<li>
<p>随着迭代进行，这种**高估（Overestimation）**会在自举（Bootstrapping）过程中不断累积，导致策略次优甚至震荡。</p>
</li>
</ul>
<hr>
<h3 id="2-解决方案：Double-Q-learning-的数学形式">2. 解决方案：Double Q-learning 的数学形式</h3>
<p>Double Q-learning 引入了两个独立的价值函数估计器 $Q_1$ 和 $Q_2$（通常参数分别为 $\theta_1, \theta_2$）。</p>
<p>在更新 $Q_1$ 时，其 TD Target 计算如下：</p>
<p>$$Y_{DQ} = r + \gamma Q_2( s’, \underbrace{\arg\max_{a’} Q_1(s’, a’)}_{\text{Action Selection}} )$$</p>
<p>这里发生了关键的<strong>解耦</strong>：</p>
<ol>
<li>
<p><strong>动作选择（Selection）</strong>：使用 $Q_1$ 来决定哪个动作是“最好”的。令 $a^* = \arg\max_{a’} Q_1(s’, a’)$。</p>
</li>
<li>
<p><strong>动作评估（Evaluation）</strong>：使用 $Q_2$ 来计算这个被选中的动作 $a^*$ 的价值。</p>
</li>
</ol>
<hr>
<h3 id="3-为什么解耦能消除偏差？">3. 为什么解耦能消除偏差？</h3>
<p>我们需要证明 $\mathbb{E}[Y_{DQ}]$ 不存在正向偏差。</p>
<p>假设 $Q_1$ 和 $Q_2$ 的噪声是相互独立的（在深度学习中通过使用 Target Network 滞后更新来近似实现这一点）。</p>
<h4 id="推导">推导</h4>
<p>我们考察 $Q_2(s’, a^<em>)$ 的期望，其中 $a^</em>$ 是由 $Q_1$ 决定的：</p>
<p>$$\mathbb{E}[ Q_2(s’, a^<em>) ] = \mathbb{E}[ Q^</em>(s’, a^<em>) + \epsilon_{2, a^</em>} ]$$</p>
<p>由于 $Q_2$ 的噪声 $\epsilon_2$ 是均值为 0 的，且与 $Q_1$（即 $a^*$ 的选择过程）独立，因此：</p>
<p>$$\mathbb{E}[ Q_2(s’, a^<em>) ] = Q^</em>(s’, a^*)$$</p>
<h4 id="关键对比">关键对比</h4>
<ul>
<li>
<p><strong>Single Q-learning</strong>: 即使 $a^<em>$ 选错了（因为 $Q_1$ 的噪声 $\epsilon_{1, a^</em>}$ 很大），我们也直接使用了这个包含大噪声的值 $Q_1(s’, a^*)$ 来更新，导致正向偏差。</p>
</li>
<li>
<p><strong>Double Q-learning</strong>: 如果 $Q_1$ 因为正向噪声 $\epsilon_{1}$ 很大而错误地认为 $a^<em>$ 是最优动作，我们转而询问 $Q_2$ 对 $a^</em>$ 的看法。由于 $Q_2$ 的噪声与 $Q_1$ 独立，<strong>$Q_2(s’, a^*)$ 大概率不会同时也包含巨大的正向噪声</strong>。实际上，它是一个无偏估计。</p>
</li>
</ul>
<p>因此，Double Q-learning 的估计近似满足：</p>
<p>$$\mathbb{E} [ Q_2(s’, \arg\max_{a’} Q_1(s’, a’)) ] \le \max_a Q^*(s’, a)$$</p>
<p>这就消除了系统性的高估误差。实际上，Double Q-learning 有时甚至会产生轻微的低估（Underestimation），但在强化学习中，低估通常比高估更稳定，因为它不会导致智能体盲目自信地陷入局部极值。</p>
<hr>
<h3 id="4-总结">4. 总结</h3>
<p>从数学结构上看，Double Q-learning 将算子 $\max_a Q(a)$ 拆解为 $Q_B(\arg\max_a Q_A(a))$。</p>
<ul>
<li>
<p><strong>Q-learning</strong>：$\mathbb{E}[\max(\cdot)]$，这是一个<strong>凸函数</strong>，导致正向偏差。</p>
</li>
<li>
<p><strong>Double Q-learning</strong>：利用两个独立统计量，使得选择过程产生的偏差无法传递到评估过程中。</p>
</li>
</ul>
<p>这就像在很多学术会议的审稿机制中：如果作者（$Q_1$）自己声称这篇论文是最好的（$\max$），往往有偏差；但如果让作者选出他认为最好的论文，然后由独立的评审人（$Q_2$）来打分，结果通常更客观。</p>

                </div>
                

                
                <div class="mdui-divider"></div>
                <div class="mdui-card-actions mdui-p-x-3 mdui-p-y-2">
                    <div class="mdui-row">
                        <div class="mdui-col-sm-3 mdui-text-left">
                            
                                <a href="/%E7%AC%94%E8%AE%B0/Transformer/" class="mdui-ripple mdui-hoverable mdui-p-a-1 mdui-text-truncate" style="display: block; text-decoration: none;">
                                    <div class="mdui-typo-caption mdui-text-color-theme-accent mdui-text-truncate">
                                        <i class="mdui-icon material-icons" style="font-size: 14px; vertical-align: middle;">arrow_back</i> 
                                        Previous
                                    </div>
                                    <div class="mdui-typo-body-1 mdui-m-t-1 mdui-text-truncate mdui-text-color-black" style="font-weight: 500;">
                                        笔记 | Transformer
                                    </div>
                                </a>
                            
                        </div>
                        <div class="mdui-col-sm-6"></div>
                        <div class="mdui-col-sm-3 mdui-text-right">
                            
                                <a href="/%E7%AC%94%E8%AE%B0/javascript%E5%9D%91/" class="mdui-ripple mdui-hoverable mdui-p-a-1 mdui-text-truncate" style="display: block; text-decoration: none;">
                                    <div class="mdui-typo-caption mdui-text-color-theme-accent mdui-text-truncate">
                                        Next
                                        <i class="mdui-icon material-icons" style="font-size: 14px; vertical-align: middle;">arrow_forward</i>
                                    </div>
                                    <div class="mdui-typo-body-1 mdui-m-t-1 mdui-text-truncate mdui-text-color-black" style="font-weight: 500;">
                                        笔记 | javascript坑
                                    </div>
                                </a>
                            
                        </div>
                    </div>
                </div>
                
                
            </div>
            
        </div>
        <!-- 侧边栏 -->
        <div class="mdui-col-md-3 mdui-col-xs-12  mdui-p-t-1 mdui-p-b-4 sticky-sidebar" id="post-sidebar">
            
                <div class="mdui-card mdui-shadow-1 toc-full-height">
                    <div class="mdui-card-content toc-card-content">
                        <div class="mdui-typo-heading mdui-m-b-2">Table of Contents</div>
                        <div class="toc-content">
                            <ol class="mdui-list mdui-typo"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="mdui-list mdui-typo-text">0. 强化学习的定义</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#1-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95-Monte-Carlo-Methods"><span class="mdui-list mdui-typo-text">1. 蒙特卡洛方法 (Monte Carlo Methods)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#2-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%8E%A7%E5%88%B6%EF%BC%9ASARSA-On-policy-TD-Control"><span class="mdui-list mdui-typo-text">2. 时序差分控制：SARSA (On-policy TD Control)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#3-Q%E5%AD%A6%E4%B9%A0-Q-learning"><span class="mdui-list mdui-typo-text">3. Q学习 (Q-learning)</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-1-%E6%8F%92%E5%85%A5%EF%BC%9A%E5%9C%A8%E7%AD%96%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A6%BB%E7%AD%96%E7%AE%97%E6%B3%95"><span class="mdui-list mdui-typo-text">3.1. 插入：在策算法与离策算法</span></a></li></ol></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#5-%E5%8F%8C%E9%87%8D-Q-%E5%AD%A6%E4%B9%A0-Double-Q-learning"><span class="mdui-list mdui-typo-text">5. 双重 Q 学习 (Double Q-learning)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#1-%E9%99%84%E5%BD%95%EF%BC%9A%E5%9C%A8%E7%AD%96%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A6%BB%E7%AD%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="mdui-list mdui-typo-text">-1. 附录：在策算法与离策算法的区别</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BD%A2%E5%BC%8F"><span class="mdui-list mdui-typo-text">1. 算法更新规则的数学形式</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E9%A3%8E%E9%99%A9%E5%81%8F%E5%A5%BD%E7%9A%84%E6%95%B0%E5%AD%A6%E6%9C%AC%E8%B4%A8%EF%BC%9A%E6%9C%9F%E6%9C%9B%E5%80%BC%E7%9A%84%E5%B1%95%E5%BC%80"><span class="mdui-list mdui-typo-text">2. 风险偏好的数学本质：期望值的展开</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E7%BB%93%E8%AE%BA%E5%88%86%E6%9E%90"><span class="mdui-list mdui-typo-text">3. 结论分析</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E6%80%BB%E7%BB%93%EF%BC%9A%E6%94%B6%E6%95%9B%E6%80%A7%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="mdui-list mdui-typo-text">4. 总结：收敛性与应用</span></a></li></ol></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#2-%E6%9C%80%E5%A4%A7%E5%8C%96%E5%81%8F%E5%B7%AE"><span class="mdui-list mdui-typo-text">-2. 最大化偏差</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E9%97%AE%E9%A2%98%E7%9A%84%E6%A0%B9%E6%BA%90%EF%BC%9A%E5%8D%95%E4%B8%80%E4%BC%B0%E8%AE%A1%E9%87%8F%E7%9A%84%E6%9C%80%E5%A4%A7%E5%8C%96%E5%81%8F%E5%B7%AE"><span class="mdui-list mdui-typo-text">1. 问题的根源：单一估计量的最大化偏差</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9ADouble-Q-learning-%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BD%A2%E5%BC%8F"><span class="mdui-list mdui-typo-text">2. 解决方案：Double Q-learning 的数学形式</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A7%A3%E8%80%A6%E8%83%BD%E6%B6%88%E9%99%A4%E5%81%8F%E5%B7%AE%EF%BC%9F"><span class="mdui-list mdui-typo-text">3. 为什么解耦能消除偏差？</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#4-%E6%80%BB%E7%BB%93"><span class="mdui-list mdui-typo-text">4. 总结</span></a></li></ol></li></ol>
                        </div>
                    </div>
                </div>
            
            <div class="mdui-m-t-2">
                <div class="mdui-card mdui-shadow-1">
  <div class="mdui-card-media" style="height: 120px; overflow: hidden;">
    <img src="/image/card.png" class="mdui-img-fluid" style="width: 100%; height: 100%; object-fit: cover;">
  </div>

  <div class="mdui-card-content mdui-text-center" style="position: relative;">
    
      <img src="/image/avatar.jpeg" class="mdui-img-circle mdui-shadow-4 sidebar-avatar-offset" style="width: 80px; height: 80px; border: 2px solid white; display: block; margin-left: auto; margin-right: auto;">
    

    <div class="mdui-typo-headline mdui-m-t-2">NaN / Not a Name</div>
    <div class="mdui-typo-caption mdui-text-color-grey-600 mdui-m-t-1">
      Nomen a Nullo. Nihil ad Nomen.
    </div>
  </div>

  <!-- <div class="mdui-divider"></div>

  <div class="mdui-row mdui-text-center mdui-p-y-2">
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">68</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">posts</div>
    </div>
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">6</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">Categories</div>
    </div>
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">0</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">Tags</div>
    </div>
  </div>

  <div class="mdui-divider"></div> -->

  <div class="mdui-card-actions mdui-text-center">
    
        
            <a href="mailto:your@email.com" target="_blank" class="mdui-btn mdui-btn-icon mdui-ripple mdui-text-color-blue" mdui-tooltip="{content: 'Email'}">
                <i class="mdui-icon material-icons">email</i>
            </a>
        
            <a href="https://github.com/yourusername" target="_blank" class="mdui-btn mdui-btn-icon mdui-ripple mdui-text-color-black" mdui-tooltip="{content: 'Github'}">
                <i class="mdui-icon material-icons">code</i>
            </a>
        
    
  </div>
</div>
            </div>
        </div>
    </div>
</div>
<div id="page-bottom" aria-hidden="true"></div>
  </div>
  <!-- Footer -->
  <footer class="mdui-color-grey-200 mdui-text-center mdui-typo-caption">
  <div class="mdui-container">
    <div class="mdui-p-y-2">
      &copy; 2026 <span class="mdui-text-color-theme">NaN / Not a Name</span>
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io/" class="mdui-text-color-theme-accent">Hexo</a> | <a target="_blank" rel="noopener" href="https://www.mdui.org/" class="mdui-text-color-theme-accent">MDUI v1</a>
    </div>
  </div>
</footer>

  <!-- Scripts -->
  <script src="/mdui/js/mdui.min.js"></script>
  <script src="/katex/katex.min.js"></script>
  <script src="/katex/contrib/auto-render.min.js"></script>
  
  <script>
    // 合并所有 DOMContentLoaded 监听器
    document.addEventListener("DOMContentLoaded", function() {
      // KaTeX 数学公式渲染
      renderMathInElement(document.body, {
        // 自定义分隔符，支持 $...$ 和 $$...$$
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ],
        // 忽略的标签
        ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
      });

      // 自动为所有表格和图片添加 MDUI 样式类
        var tables = document.getElementsByTagName("table");
        for (var i = 0; i < tables.length; i++) {
            if (!tables[i].classList.contains("mdui-table")) {
                tables[i].classList.add("mdui-table", "mdui-shadow-0");
            }
        }
        var imgs = document.getElementsByTagName("img");
        for (var i = 0; i < imgs.length; i++) {
            if (!imgs[i].classList.contains("mdui-img-fluid")) {
                imgs[i].classList.add("mdui-img-fluid");
            }
        }
    });
  </script>
</body>
</html>