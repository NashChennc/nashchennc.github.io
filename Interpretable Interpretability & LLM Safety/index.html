<!DOCTYPE html>
<html lang="cn">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Interpretable Interpretability &amp; LLM Safety - NaN&#39;s Blog ｜ Not a Nihilist</title>

  <!-- MDUI CSS -->
  <link rel="stylesheet" href="/mdui/css/mdui.min.css">

  <!-- KaTeX CSS (If needed) -->
  <link rel="stylesheet" href="/katex/katex.min.css">

  <!-- Custom CSS (最小化，只包含无法用 MDUI 实现的部分) -->
  <link rel="stylesheet" href="/css/custom.css">

  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <meta name="theme-color" content="#3F51B5">
<meta name="generator" content="Hexo 8.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
<body class="mdui-appbar-with-toolbar mdui-theme-primary-indigo mdui-theme-accent-pink mdui-color-grey-100">
  
  <!-- Header/AppBar -->
  <header class="appbar mdui-appbar mdui-appbar-fixed">
    <div class="mdui-toolbar mdui-color-theme">
        
          <div class="mdui-toolbar-spacer"></div>
            <div class="mdui-tab mdui-tab-full-width">
                
                <a href="/" class="mdui-ripple ">
                    Home
                </a>
                
                
                    
                        <a href="/categories/笔记" class="mdui-ripple ">
                            笔记 | 9
                        </a>
                    
                
                    
                        <a href="/categories/科研" class="mdui-ripple ">
                            科研 | 7
                        </a>
                    
                
                    
                        <a href="/categories/碎碎念" class="mdui-ripple ">
                            碎碎念 | 39
                        </a>
                    
                
                    
                        <a href="/categories/工具箱" class="mdui-ripple ">
                            工具箱 | 16
                        </a>
                    
                
                    
                        <a href="/categories/专栏" class="mdui-ripple ">
                            专栏 | 3
                        </a>
                    
                
            </div>
        
        <div class="mdui-toolbar-spacer"></div>
        <a href="/about" class="mdui-btn mdui-btn-icon mdui-ripple" mdui-tooltip="{content: 'About'}">
            <i class="mdui-icon material-icons"> more_vert </i>
        </a>
    </div>
</header>

  <!-- Main Content (Layout control moved to individual views) -->
  <div id="main-content">
    <!-- 全宽 Banner -->
<div class="banner-container" style="height: 350px;">
    <div class="banner-bg" style="background-image: url(/image/banner.png);"></div>
    
    <div class="mdui-card-media-covered banner-overlay" style="background: rgba(0, 0, 0, 0.4);">
        <div class="mdui-container">
            <div class="mdui-card-primary mdui-float-right mdui-text-color-white mdui-p-y-2">
                <div class="mdui-card-primary-title" style="font-weight: 300;">
                    NaN&#39;s Blog ｜ Not a Nihilist
                </div>
                <div class="mdui-card-primary-subtitle mdui-float-right" style="opacity: 0.9;">
                    Nomen a Nullo. Nihil ad Nomen.
                </div>
            </div>
        </div>
    </div>
</div>
<!-- FAB -->
<div id="exampleFab" class="mdui-fab-wrapper" mdui-fab="{trigger: 'hover'}">
    <a class="mdui-fab mdui-ripple mdui-color-theme-accent" href="/archives" mdui-tooltip="{content: 'goto Archive', position: 'left'}">
        <i class="mdui-icon material-icons">bookmark</i>
        <i class="mdui-icon mdui-fab-opened material-icons">bookmark</i>
    </a>
    <div class="mdui-fab-dial">
        <a href="/about" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-teal">
            <i class="mdui-icon material-icons">more_vert</i>
        </a>
        <a href="#top" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-blue-700" mdui-tooltip="{content: 'goto top', position: 'left'}">
            <i class="mdui-icon material-icons">keyboard_arrow_up</i>
        </a>
        <a href="#page-bottom" class="mdui-fab mdui-fab-mini mdui-ripple mdui-color-indigo" mdui-tooltip="{content: 'goto bottom', position: 'left'}">
            <i class="mdui-icon material-icons">keyboard_arrow_down</i>
        </a>
    </div>
</div>

<div class="mdui-container">
    <div class="mdui-row mdui-p-t-1 mdui-p-b-1">
        <!-- 文章内容 -->

        <div class="mdui-col-md-9 mdui-col-xs-12 mdui-p-t-1 mdui-p-b-4">
            <div class="mdui-card mdui-hoverable">
                <div class="mdui-card-primary mdui-p-x-5">
                    <div class="mdui-card-primary-title mdui-typo-display-1">
                        Interpretable Interpretability & LLM Safety
                    </div>
                    <div class="mdui-row mdui-p-t-2">
                        <div class="mdui-col-xs-9 mdui-typo-caption-opacity">
                            Last Update: 2026/01/18 02:37
                        </div>
                        <div class="mdui-col-xs-3">
                            <div class="mdui-float-right mdui-typo">
                                
                                    <a href="/categories/%E7%A7%91%E7%A0%94/">
                                        科研
                                    </a>
                                
                            </div>
                        </div>
                    </div>
                    <div class="mdui-divider"></div>
                    <br/>
                </div>
                <div class="mdui-card-content mdui-typo mdui-p-x-5">
                    <h1>Interpretable Interpretability &amp; LLM Safety</h1>
<p>研究如何使用 <strong>可解释的手段</strong> 影响 LLM 的安全性。</p>
<h2 id="一些-llm-基础">一些 LLM 基础</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">2017.06: Transformer</a> 过于简短，建议自行上完搜教程</p>
</li>
<li>
<p>基于 Transformer 的 LLM：可以按 Transformer Lens 的教程自己写一遍 GPT2</p>
</li>
<li>
<p>MoE 模型</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">2021.06: LoRA</a></p>
</li>
</ul>
<p>NLP 的一些基础可能已经过时，但也许会有一点启发，有助于理解。</p>
<p>详见 Stanford CS224n</p>
<h2 id="mi-alignment-资料">MI &amp; Alignment 资料</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Getting Start</a></p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1">2024.07: interpretability 入门清单</a></p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://www.notion.so/Alignment-Guidebook-e5c64df77c0a4b528b7951e87337fa78#a9a397ec7bbf4c4486334383f9087468">Alignment Guidebook</a></p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://transformerlensorg.github.io/TransformerLens/content/getting_started.html">Transformer Lens Docs</a></p>
</li>
</ul>
<h1>记录</h1>
<h2 id="2024-10-09">2024.10.09</h2>
<h3 id="相关研究">相关研究</h3>
<p>LLM 可解释安全的一些处理方法</p>
<ul>
<li>
<p><strong>打算 follow 的工作</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.01967">2024.01: DPO and Toxicity</a> 以 ==GPT2-DPO== 为例研究了对齐算法 ==(DPO)== 对预训练模型 ==(GPT2)== 的影响。这篇用到了 Google 的 Perspective API，弄账号有点麻烦，API 响应也很慢。跑代码时记得加上代理。DPO 代码是在 ==Linux== 上写的，一些库不支持 ==Windows==</p>
</li>
<li>
<p>Pruning &amp; LoRA <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.05162">2024.02: Pruning and LoRA</a></p>
</li>
<li>
<p>Residual Stream <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.11717">2024.06: Refusal Direction</a> 这篇提供了 Colab 代码，操作比较方便。</p>
</li>
<li>
<p>Attention Head <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.13708">2024.10: Attention Head</a> (周振宏/方俊峰师兄)</p>
</li>
</ul>
<p>关于 LLM 推理特性的理解</p>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.01552">2023.12: Urial</a> 使用提示词工程揭示了关于 LLM 对齐的一些特性</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://aclanthology.org/2022.emnlp-main.3.">2022.03: Logit Lens</a> 中间层语义</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.05644">2024.06: Intermediate Layers</a> 词表在前向传播中有连续变化的规律 (周振宏师兄)</p>
</li>
</ul>
<h2 id="2024-12-25-sae">2024.12.25 SAE</h2>
<h3 id="transformer-circuits-thread-anthropic">Transformer Circuits Thread @Anthropic</h3>
<p>Anthropic 的一个研究项目。项目主要目标是将 transformer 语言模型逆向工程为人类可理解的计算机程序。</p>
<p>项目资源链接中有相关研究和资源，在网页论文上有一些制作精美的展示，比起传统会议的文章看着舒服。<a target="_blank" rel="noopener" href="https://transformer-circuits.pub">Transformer Circuits Thread</a> | <a target="_blank" rel="noopener" href="https://www.anthropic.com/">Anthropic 官网</a></p>
<p>Sparse AutoEncoder, SAE</p>
<h3 id="sae-基础">SAE 基础</h3>
<p>一些关键基础研究，推动了 SAE 在 LLM 上的使用</p>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2021/framework/index.html">2021.12 A Mathematical Framework for Transformer Circuits</a> 注意力层形成的聚合结构</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2022/toy_model/index.html">2022.09 Toy Models of Superposition</a> 大量语义向量在低维线性空间的表征</p>
</li>
</ul>
<h3 id="基本结构以及一些成品">基本结构以及一些成品</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">2023.12 Towards Monosemanticity (miniSAE)</a> 层数较少模型中的 SAE</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">2024.05 Scaling monosemanticity (SAE)</a> 大模型的 SAE</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.04093">2024.06 Scaling and evaluating sparse autoencoders</a> OpenAI: TopK SAE &amp; Scaling laws</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.16014">2024.04 Gated SAE (Deepmind)</a> Deepmind: Gated SAE</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.20526">2024.10 SAE on Llama 3.1</a> 留意 Github <a target="_blank" rel="noopener" href="https://github.com/OpenMOSS/Language-Model-SAEs">OpenMOSS/Language-Model-SAEs</a></p>
</li>
</ul>
<h3 id="sae-使用例">SAE 使用例</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.19647">2024.03 Sparse Feature Circuits</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.14257">2024.11 Entity Recognize &amp; SAE</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2409.04185">2024.10 Residual Stream</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.05276">2024.12 selective remapping</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.07108">2025.01 黑白棋 GPT</a></li>
</ul>
<h3 id="未来的变体-2024-10">未来的变体 2024.10</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2024/features-as-classifiers/index.html">2024.10 features as classifiers</a> SAE + 分类器</li>
<li><a target="_blank" rel="noopener" href="https://transformer-circuits.pub/2024/crosscoders/index.html">2024.10 crosscoders</a> 跨层 SAE</li>
</ul>
<h3 id="openmoss-language-model-saes">OpenMOSS/Language-Model-SAEs</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/OpenMOSS/Language-Model-SAEs">OpenMOSS/Language-Model-SAEs</a></li>
<li><a target="_blank" rel="noopener" href="https://www.neuronpedia.org/">neuronpedia</a></li>
</ul>
<p>过采样某领域数据集 / system prompt 产生的不同 SAE</p>
<p>不同 SAE / 不同层、不同模型甚至不同架构的 SAE？</p>
<p>如何比较 / 干预（修改 SAE，可能可以某些 SAE 互补，但其实这个可能不太有用且不太好做） / SAE 训练中持续干预（定向训练 SAE）</p>
<p>dpo 前后 SAE 通用？每一层重建损失</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">数据集</a></p>
<h2 id="iclr2025">ICLR2025</h2>
<p><a target="_blank" rel="noopener" href="https://openreview.net/group?id=ICLR.cc/2025/Conference">OpenReview | ICLR2025</a></p>
<h3 id="oral">Oral</h3>
<ul>
<li>(10/10/8/3) Scaling and evaluating sparse autoencoders</li>
</ul>
<p>OpenAI 的 TopK SAE，之前已经看过</p>
<ul>
<li>(8/8/8/8) Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</li>
</ul>
<p>之前提到过的 用 SAE feature 重新做 Circuits</p>
<h3 id="10-10-8-8-do-i-know-this-entity-knowledge-awareness-and-hallucinations-in-language-models">(10/10/8/8) Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</h3>
<ul>
<li>找到 “是否识别某个实体” 的 SAE feature。</li>
<li>“已知实体”：回答正确两个关于该实体的信息 / “未知实体”：两个都回答错</li>
<li>这种方向会影响模型的 <strong>拒绝行为</strong>，引导模型拒绝回答关于已知实体的问题，或者在原本会拒绝的情况下虚构未知实体的属性。</li>
<li>会干扰和该实体有关的下游注意力头</li>
<li><strong>用基础模型训出来的 SAE 在聊天模型能用</strong></li>
<li>找了表示不确定性的 SAE feature，可以用来预测幻觉</li>
</ul>
<h3 id="poster">Poster</h3>
<h3 id="8-8-6-6-towards-principled-evaluations-of-sparse-autoencoders-for-interpretability-and-control">(8/8/6/6) Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control</h3>
<h3 id="ioi-task-间接宾语测试">IOI task 间接宾语测试</h3>
<p>SAE 的评估框架。</p>
<ul>
<li>将 SAE 和有监督字典比较，可以用于评估 SAE。</li>
<li>选择一些任务，评估两者相似性、两者的稀疏可控性和可解释性三个方面的表现。有监督字典对模型的控制比 SAE 强。</li>
</ul>
<p>发现 SAE 的一些问题</p>
<ul>
<li>特征遮挡: （激活和 feature 的一些关系）同一激活表示两个相关属性，某属性 feature 较大时另一属性会被覆盖</li>
<li>特征过度分割</li>
</ul>
<p>很多人会建小模型来做实验，可以降低开销</p>
<h3 id="8-8-6-6-sparse-autoencoders-do-not-find-canonical-units-of-analysis">(8/8/6/6) Sparse Autoencoders Do Not Find Canonical Units of Analysis</h3>
<p>认为 SAE 在寻找完整特征和原子特征方面存在不足。</p>
<p>提出了一些操作 SAE 的方法：</p>
<p>基于解码器方向之间的余弦相似度，将较大 SAE 的特征分为两类：</p>
<ul>
<li>新特征: 与较小 SAE 中的特征的余弦相似度低于阈值的特征。</li>
<li>重建特征: 与较小 SAE 中某些特征具有高余弦相似度的特征。</li>
</ul>
<p>拼接 SAE: 将新颖特征添加到较小的 SAE 中 / 将重建特征与较小 SAE 中具有相似行为的特征进行交换，减少冗余并进一步提高重建性能。</p>
<p>Meta SAE：在 SAE 上训 SAE，得到一些 feature cluster</p>
<p>用这些方法进行了实验，比较有效，并以此质疑原 SAE 的效果</p>
<h3 id="6-6-6-5-rethinking-evaluation-of-sparse-autoencoders-through-the-representation-of-polysemous-wordsí">(6/6/6/5) Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Wordsí</h3>
<ul>
<li>(8/6/6/6) Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures</li>
<li>上次 Llama scope 团队的</li>
<li>Not All Language Model Features Are Linear</li>
<li>Residual Stream Analysis with Multi-Layer SAEs</li>
<li>Sparse autoencoders reveal selective remapping of visual concepts during adaptation</li>
</ul>
<p>还没看的</p>
<ul>
<li>(8/6/6/5) Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models</li>
<li>(5/8/3/3/5) Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers</li>
<li>Mechanistic Permutability: Match Features Across Layers</li>
<li>Monet: Mixture of Monosemantic Experts for Transformers</li>
<li>(8/8/6/6) Efficient Dictionary Learning with Switch Sparse Autoencoders 用类似 MoE 的方法改进 SAE 的训练效率. 感觉工程上有点复杂，就暂时没看</li>
</ul>
<h3 id="任务">任务</h3>
<p>具体结合 deepseek 的 SAE<a target="_blank" rel="noopener" href="https://huggingface.co/qresearch/DeepSeek-R1-Distill-Llama-70B-SAE-l48">https://huggingface.co/qresearch/DeepSeek-R1-Distill-Llama-70B-SAE-l48</a></p>
<p>对齐前后模型 / 对齐前后 SAE</p>
<p>余弦相似度 / 重建损失</p>
<p>蒸馏相关 / 推理相关</p>
<h2 id="20250314">20250314</h2>
<p>A40 单卡跑8B模型+大约<a target="_blank" rel="noopener" href="https://aideadlin.es/?sub=ML,NLP,SP,KR,HCI,DM">https://aideadlin.es/?sub=ML,NLP,SP,KR,HCI,DM</a><a target="_blank" rel="noopener" href="https://ccfddl.com">https://ccfddl.com</a></p>
<h2 id="20250425">20250425</h2>
<p><a target="_blank" rel="noopener" href="https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens">利用富集度提取特征</a></p>
<h2 id="202512">202512</h2>
<p><a target="_blank" rel="noopener" href="https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well">https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well</a></p>
<ul>
<li>202502 Are SAE Useful？ <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.16681">https://arxiv.org/abs/2502.16681</a></li>
<li>202512 sae for embedding search <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.10092v1">https://arxiv.org/abs/2512.10092v1</a></li>
<li>202509 iclr26 高分被拒 SAE reconstruction for OOD detection <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=F6DzptrpGI">https://openreview.net/forum?id=F6DzptrpGI</a></li>
<li>202509 Target Aligned Data Filtering <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=cgmo3v18sx">https://openreview.net/forum?id=cgmo3v18sx</a></li>
<li>202411 Decoding Dark Matter <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.00743v1">https://arxiv.org/abs/2411.00743v1</a></li>
<li>202410 Sparse Autoencoder Feature Structure <a target="_blank" rel="noopener" href="https://arxiv.org/html/2410.19750v1">https://arxiv.org/html/2410.19750v1</a> 分析数据集</li>
<li>202508 Domain-Specific SAE <a target="_blank" rel="noopener" href="https://arxiv.org/html/2508.09363v1">https://arxiv.org/html/2508.09363v1</a></li>
<li>202509 Fine-grained Safety <a target="_blank" rel="noopener" href="https://arxiv.org/html/2509.18127v1">https://arxiv.org/html/2509.18127v1</a></li>
<li>202411 steering refusal <a target="_blank" rel="noopener" href="https://arxiv.org/html/2411.11296v1">https://arxiv.org/html/2411.11296v1</a></li>
<li>202507 Delta SAE <a target="_blank" rel="noopener" href="https://arxiv.org/html/2507.12990v1">https://arxiv.org/html/2507.12990v1</a></li>
<li>202409 refusal features <a target="_blank" rel="noopener" href="https://arxiv.org/html/2409.20089v2">https://arxiv.org/html/2409.20089v2</a>
SAE重构误差还可以用于检测数据集中的对抗性攻击或“越狱”样本。攻击者注入的恶意Prompt往往会触发异常的内部激活模式（例如，激活幅度远超正常范围，或激活了互斥的特征组合）。通过监控SAE的重构误差以及特征激活的稀疏度（L0），研究者可以有效地筛选出这些潜在的“毒数据”，防止模型在训练中习得有害行为。</li>
</ul>
<h2 id="20251028-industrial-sae-for-pii-detection">20251028 Industrial: SAE for PII detection</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.goodfire.ai/research/rakuten-sae-probes-for-pii-detection#">https://www.goodfire.ai/research/rakuten-sae-probes-for-pii-detection#</a></li>
</ul>
<h3 id="1-标签的具体类别-classes">1. 标签的具体类别 (Classes)</h3>
<p>文章明确定义了每一个 token（词元）被分类的目标集合。模型并非对整句话打标签，而是对序列中的每一个 token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span> 进行预测。</p>
<p>具体的标签集合 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span> 为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">{</mo><mtext mathvariant="monospace">personal_name</mtext><mo separator="true">,</mo><mtext mathvariant="monospace">address</mtext><mo separator="true">,</mo><mtext mathvariant="monospace">phone</mtext><mo separator="true">,</mo><mtext mathvariant="monospace">email</mtext><mo separator="true">,</mo><mtext mathvariant="monospace">none</mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">p \in \{ \texttt{personal\_name}, \texttt{address}, \texttt{phone}, \texttt{email}, \texttt{none} \}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord text"><span class="mord texttt">personal_name</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord texttt">address</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord texttt">phone</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord texttt">email</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord texttt">none</span></span><span class="mclose">}</span></span></span></span></span></p>
<p>这意味着每一个 token 都会被标记为上述 5 种类别之一（具体的 PII 类型或者非 PII）。</p>
<p>文章在 “Formal setup” 章节中描述了如何对每一个 token 进行操作。</p>
<ul>
<li>
<p><strong>输入</strong>: 对于输入序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span> 中的每一个 token <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span>。</p>
</li>
<li>
<p><strong>提取</strong>: 从 Sidecar 模型（Llama 3.1 8B）的中间层 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span> 提取激活值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span></span></span></span>。</p>
</li>
<li>
<p><strong>预测</strong>: 分类器（Probe）针对该 token 输出预测结果。</p>
<ul>
<li>对于 <strong>SAE Probe</strong>，它使用 SAE 特征激活值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span></span></span></span> 作为输入：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">E</mi></mrow></msub><mo>=</mo><msub><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">E</mi></mrow></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\mathrm{SAE}} = \mathrm{clf}_\mathrm{SAE}(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">S</span><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">c</span><span class="mord mathrm">l</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">S</span><span class="mord mathrm mtight">A</span><span class="mord mathrm mtight">E</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span>。</li>
<li>目标就是预测该 token 对应的标签 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>。</li>
</ul>
</li>
<li>
<p>由于真实 PII 数据的敏感性，训练数据是<strong>全合成的 (entirely synthetic)</strong>。</p>
</li>
<li>
<p>这意味着训练时的 token level 标签是由生成合成数据时自动标注的。</p>
</li>
<li>
<p>文章的一个核心挑战是：在合成数据的 token 标签上训练，通过 SAE Probe 泛化到真实生产环境数据（Real production data）的 token 标签预测上。</p>
</li>
</ul>
<h3 id="2-random-forest-for-sae-probing">2. Random Forest for SAE probing</h3>
<p>We found that our SAE probes outperformed our activation probes and all attention probes. In particular, <strong>our SAE probes were considerably more robust across the distribution shift from synthetic to real data.</strong></p>
<p>随机森林属于 <strong>Bagging (Bootstrap Aggregating)</strong> 算法。</p>
<ul>
<li><strong>原理：</strong> 想象你要做一个重大决定，你不想一个人做，而是找了 100 个专家（决策树）来投票。
<ol>
<li><strong>随机性 (Randomness)：</strong> 每个专家只看<strong>一部分数据</strong>（有放回抽样），并且只关注<strong>一部分特征</strong>。这样保证了每个专家都有差异，不会犯同样的错。</li>
<li><strong>并行 (Parallel)：</strong> 这 100 个专家是互不干扰、同时训练的。</li>
<li><strong>投票 (Voting)：</strong> 最终结果由多数决（分类）或平均值（回归）决定。</li>
</ol>
</li>
<li><strong>为什么适合 SAE Probe？</strong>
<ul>
<li>SAE 特征通常是<strong>高维且稀疏</strong>的（成千上万个特征，大部分是 0）。随机森林在分裂节点时随机选取特征子集，非常擅长在高维噪音中找到有用的信号，且不容易过拟合。</li>
</ul>
</li>
</ul>
<h3 id="3-sae-探针">3. SAE 探针</h3>
<p>We were surprised to find that the SAE probes outperformed the activation probes at all. Prior work suggests that probing with SAE features does not usually outperform probing with raw activations (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.16681">Kantamneni et al., 2025</a>) and in particular does not generalize as well out of distribution (<a target="_blank" rel="noopener" href="https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft">Smith et al., 2025</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.03407">Goldowsky-Dill et al., 2025</a>). However, <a target="_blank" rel="noopener" href="https://www.tilderesearch.com/blog/sieve">Karvonen et al., 2024</a> provide an example in which SAEs do outperform activation-probe baselines (in a data-limited setting).
我们惊讶地发现，SAE 探针的性能竟然优于激活探针。先前的研究表明，使用 SAE 特征进行探测通常不如使用原始激活进行探测（Kantamneni 等人，2025），尤其是在分布外泛化能力较差（Smith 等人，2025；Goldowsky-Dill 等人，2025）。然而，Karvonen 等人（2024）提供了一个例子，表明在数据受限的情况下，SAE 的性能确实优于激活探针基线。</p>
<p>相比之下，在我们几乎所有的实验中，SAE 探针都超过了激活探针（主要例外是仅包含英文数据）。</p>
<p>我们对我们的研究结果与以往大多数研究结果存在差异的原因有以下几点假设：</p>
<ul>
<li>我们研究的合成分布到真实分布的转变可能与先前研究中的分布转变有所不同（尤其是更大）。</li>
<li>我们的 SAE 使用了比以往工作（大多为 32）更小的扩展因子（8）。虽然目前还没有广泛认可的扩展因子选择方法，但我们预期，当使用小型探测训练数据集时，较大的扩展因子会导致更多的特征分裂和较差的 OOD 泛化能力。</li>
<li>我们的任务涉及位置感知标记级分类，而 Kantamneni 等人和 Smith 等人则使用最大池化 SAE 研究了整体提示分类任务。</li>
<li>我们的任务包含大量日语数据——日语是 Llama 3.1 8B 不支持或资源匮乏的语言——而以往的研究仅考虑英语任务。有关不同语言的性能图表，请参见附录 D.3。</li>
</ul>
<p>See <a target="_blank" rel="noopener" href="https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft#Is_it_surprising_that_SAEs_didn_t_work_">Smith et al.</a> for additional discussion of when SAEs may work better or worse for downstream tasks.<br>
有关 SAE 在下游任务中何时可能表现更好或更差的更多讨论，请参阅 Smith 等人的文章。</p>
<h4 id="20260116报告">20260116报告</h4>
<p>SAE等LLM逆向工程比起CoT的优势在哪？我们做实验的时候发现在SAE特征解释中的两个问题：</p>
<ul>
<li>每一个 SAE 特征的具体解释最终还是要回归到数据模式上，Logit Lens 反而相对不可信</li>
<li>LLM内部激活的几何结构可能非常复杂，现有的SAE训练方法非常不稳定。完全枚举安全特征相当困难。</li>
</ul>
<p>cot不是真正的推理，可解释也不能枚举
你们的工作还是用cot，很多人认为SAE scalable oversight有优势
cot 监督的和可解释的方法在安全里面的角色？有没有相关的工作</p>
<p>SafeWork-T1？具体做法？推理透明具体做法？你们似乎还是基于CoT
因果关系？按你们定义更像是演化推测，类似World Model。</p>
<p>vLLM 幻觉
人也有这种问题（教小孩得出的经验）具体解决方法？</p>
<p><a target="_blank" rel="noopener" href="https://www.epitome-ai.com">https://www.epitome-ai.com</a></p>
<p><a target="_blank" rel="noopener" href="https://ai45.shlab.org.cn">https://ai45.shlab.org.cn</a></p>

                </div>
                

                
                <div class="mdui-divider"></div>
                <div class="mdui-card-actions mdui-p-x-3 mdui-p-y-2">
                    <div class="mdui-row">
                        <div class="mdui-col-sm-3 mdui-text-left">
                            
                        </div>
                        <div class="mdui-col-sm-6"></div>
                        <div class="mdui-col-sm-3 mdui-text-right">
                            
                                <a href="/From%20Anthropic/" class="mdui-ripple mdui-hoverable mdui-p-a-1 mdui-text-truncate" style="display: block; text-decoration: none;">
                                    <div class="mdui-typo-caption mdui-text-color-theme-accent mdui-text-truncate">
                                        Next
                                        <i class="mdui-icon material-icons" style="font-size: 14px; vertical-align: middle;">arrow_forward</i>
                                    </div>
                                    <div class="mdui-typo-body-1 mdui-m-t-1 mdui-text-truncate mdui-text-color-black" style="font-weight: 500;">
                                        科研 | From Anthropic
                                    </div>
                                </a>
                            
                        </div>
                    </div>
                </div>
                
                
            </div>
            
        </div>
        <!-- 侧边栏 -->
        <div class="mdui-col-md-3 mdui-col-xs-12  mdui-p-t-1 mdui-p-b-4 sticky-sidebar" id="post-sidebar">
            
                <div class="mdui-card mdui-shadow-1 toc-full-height">
                    <div class="mdui-card-content toc-card-content">
                        <div class="mdui-typo-heading mdui-m-b-2">Table of Contents</div>
                        <div class="toc-content">
                            <ol class="mdui-list mdui-typo"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-1"><a class="mdui-list mdui-typo-link"><span class="mdui-list mdui-typo-text">Interpretable Interpretability &amp; LLM Safety</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#%E4%B8%80%E4%BA%9B-llm-%E5%9F%BA%E7%A1%80"><span class="mdui-list mdui-typo-text">一些 LLM 基础</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#mi-alignment-%E8%B5%84%E6%96%99"><span class="mdui-list mdui-typo-text">MI &amp; Alignment 资料</span></a></li></ol></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-1"><a class="mdui-list mdui-typo-link"><span class="mdui-list mdui-typo-text">记录</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#2024-10-09"><span class="mdui-list mdui-typo-text">2024.10.09</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6"><span class="mdui-list mdui-typo-text">相关研究</span></a></li></ol></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#2024-12-25-sae"><span class="mdui-list mdui-typo-text">2024.12.25 SAE</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#transformer-circuits-thread-anthropic"><span class="mdui-list mdui-typo-text">Transformer Circuits Thread @Anthropic</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#sae-%E5%9F%BA%E7%A1%80"><span class="mdui-list mdui-typo-text">SAE 基础</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E6%88%90%E5%93%81"><span class="mdui-list mdui-typo-text">基本结构以及一些成品</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#sae-%E4%BD%BF%E7%94%A8%E4%BE%8B"><span class="mdui-list mdui-typo-text">SAE 使用例</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E6%9C%AA%E6%9D%A5%E7%9A%84%E5%8F%98%E4%BD%93-2024-10"><span class="mdui-list mdui-typo-text">未来的变体 2024.10</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#openmoss-language-model-saes"><span class="mdui-list mdui-typo-text">OpenMOSS&#x2F;Language-Model-SAEs</span></a></li></ol></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#iclr2025"><span class="mdui-list mdui-typo-text">ICLR2025</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#oral"><span class="mdui-list mdui-typo-text">Oral</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#10-10-8-8-do-i-know-this-entity-knowledge-awareness-and-hallucinations-in-language-models"><span class="mdui-list mdui-typo-text">(10&#x2F;10&#x2F;8&#x2F;8) Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#poster"><span class="mdui-list mdui-typo-text">Poster</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#8-8-6-6-towards-principled-evaluations-of-sparse-autoencoders-for-interpretability-and-control"><span class="mdui-list mdui-typo-text">(8&#x2F;8&#x2F;6&#x2F;6) Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#ioi-task-%E9%97%B4%E6%8E%A5%E5%AE%BE%E8%AF%AD%E6%B5%8B%E8%AF%95"><span class="mdui-list mdui-typo-text">IOI task 间接宾语测试</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#8-8-6-6-sparse-autoencoders-do-not-find-canonical-units-of-analysis"><span class="mdui-list mdui-typo-text">(8&#x2F;8&#x2F;6&#x2F;6) Sparse Autoencoders Do Not Find Canonical Units of Analysis</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#6-6-6-5-rethinking-evaluation-of-sparse-autoencoders-through-the-representation-of-polysemous-words%C3%AD"><span class="mdui-list mdui-typo-text">(6&#x2F;6&#x2F;6&#x2F;5) Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Wordsí</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#%E4%BB%BB%E5%8A%A1"><span class="mdui-list mdui-typo-text">任务</span></a></li></ol></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#20250314"><span class="mdui-list mdui-typo-text">20250314</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#20250425"><span class="mdui-list mdui-typo-text">20250425</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#202512"><span class="mdui-list mdui-typo-text">202512</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-2"><a class="mdui-list mdui-typo-link" href="#20251028-industrial-sae-for-pii-detection"><span class="mdui-list mdui-typo-text">20251028 Industrial: SAE for PII detection</span></a><ol class="mdui-list mdui-typo-child"><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#1-%E6%A0%87%E7%AD%BE%E7%9A%84%E5%85%B7%E4%BD%93%E7%B1%BB%E5%88%AB-classes"><span class="mdui-list mdui-typo-text">1. 标签的具体类别 (Classes)</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#2-random-forest-for-sae-probing"><span class="mdui-list mdui-typo-text">2. Random Forest for SAE probing</span></a></li><li class="mdui-list mdui-typo-item mdui-list mdui-typo-level-3"><a class="mdui-list mdui-typo-link" href="#3-sae-%E6%8E%A2%E9%92%88"><span class="mdui-list mdui-typo-text">3. SAE 探针</span></a></li></ol></li></ol></li></ol>
                        </div>
                    </div>
                </div>
            
            <div class="mdui-m-t-2">
                <div class="mdui-card mdui-shadow-1">
  <div class="mdui-card-media" style="height: 120px; overflow: hidden;">
    <img src="/image/card.png" class="mdui-img-fluid" style="width: 100%; height: 100%; object-fit: cover;">
  </div>

  <div class="mdui-card-content mdui-text-center" style="position: relative;">
    
      <img src="/image/avatar.jpeg" class="mdui-img-circle mdui-shadow-4 sidebar-avatar-offset" style="width: 80px; height: 80px; border: 2px solid white; display: block; margin-left: auto; margin-right: auto;">
    

    <div class="mdui-typo-headline mdui-m-t-2">NaN / Not a Name</div>
    <div class="mdui-typo-caption mdui-text-color-grey-600 mdui-m-t-1">
      Nomen a Nullo. Nihil ad Nomen.
    </div>
  </div>

  <!-- <div class="mdui-divider"></div>

  <div class="mdui-row mdui-text-center mdui-p-y-2">
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">80</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">posts</div>
    </div>
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">5</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">Categories</div>
    </div>
    <div class="mdui-col-xs-4">
      <div class="mdui-typo-subheading">26</div>
      <div class="mdui-typo-caption mdui-text-color-grey-600">Tags</div>
    </div>
  </div>

  <div class="mdui-divider"></div> -->

  <div class="mdui-card-actions mdui-text-center">
    
        
            <a href="mailto:your@email.com" target="_blank" class="mdui-btn mdui-btn-icon mdui-ripple mdui-text-color-blue" mdui-tooltip="{content: 'Email'}">
                <i class="mdui-icon material-icons">email</i>
            </a>
        
            <a href="https://github.com/yourusername" target="_blank" class="mdui-btn mdui-btn-icon mdui-ripple mdui-text-color-black" mdui-tooltip="{content: 'Github'}">
                <i class="mdui-icon material-icons">code</i>
            </a>
        
    
  </div>
</div>
            </div>
        </div>
    </div>
</div>
<div id="page-bottom" aria-hidden="true"></div>
  </div>
  <!-- Footer -->
  <footer class="mdui-color-grey-200 mdui-text-center mdui-typo-caption">
  <div class="mdui-container">
    <div class="mdui-p-y-2">
      &copy; 2026 <span class="mdui-text-color-theme">NaN / Not a Name</span>
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io/" class="mdui-text-color-theme-accent">Hexo</a> | <a target="_blank" rel="noopener" href="https://www.mdui.org/" class="mdui-text-color-theme-accent">MDUI v1</a>
    </div>
  </div>
</footer>

  <!-- Scripts -->
  <script src="/mdui/js/mdui.min.js"></script>
  <script src="/katex/katex.min.js"></script>
  <script src="/katex/contrib/auto-render.min.js"></script>
  
  <script>
    // 合并所有 DOMContentLoaded 监听器
    document.addEventListener("DOMContentLoaded", function() {
      // KaTeX 数学公式渲染
      renderMathInElement(document.body, {
        // 自定义分隔符，支持 $...$ 和 $$...$$
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ],
        // 忽略的标签
        ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"]
      });

      // 自动为所有表格和图片添加 MDUI 样式类
        var tables = document.getElementsByTagName("table");
        for (var i = 0; i < tables.length; i++) {
            if (!tables[i].classList.contains("mdui-table")) {
                tables[i].classList.add("mdui-table", "mdui-shadow-0");
            }
        }
        var imgs = document.getElementsByTagName("img");
        for (var i = 0; i < imgs.length; i++) {
            if (!imgs[i].classList.contains("mdui-img-fluid")) {
                imgs[i].classList.add("mdui-img-fluid");
            }
        }
    });
  </script>
</body>
</html>